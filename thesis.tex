\documentclass[12pt,twoside,final,hidelinks]{ltthesis}
\usepackage{epsfig,bm,epsf,float,lsalike}

\usepackage{xspace}
\usepackage{relsize}
\usepackage{url}
\usepackage{boxedminipage}
\usepackage{amsmath}
\usepackage{pdfpages}
\usepackage{subfig}
\usepackage{needspace}
\usepackage{array}
\usepackage{palatino}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{rotating}
\usepackage{tablefootnote}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{lipsum}
\usepackage{breqn}
\usepackage{listings}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{fixme}
%\usepackage{natbib}
\usepackage{soul}
\usepackage{qtree}
%\usepackage{natbib}
\usepackage{bibentry}
\usepackage{tikz-dependency}
\usepackage{breakcites}
\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}

\fancyhead[RO,LE]{\thepage}
\fancyhead[LO]{\leftmark}
\fancyhead[RE]{\rightmark}


\usepackage{xcolor}
\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]
\usepackage{xcolor}
\usepackage{lipsum}

% Define macro for paper 
\newcommand\emnlpiv{EMNLP 2014 (\S\ref{sec:emnlp14})}
\newcommand\conllv{CoNLL 2015 (\S\ref{sec:conll15})}
\newcommand\aclv{ACL 2015 (\S\ref{sec:acl15})}
\newcommand\emnlpv{EMNLP 2015 (\S\ref{sec:emnlp15})}
\newcommand\naaclvi{NAACL 2016 (\S\ref{sec:naacl16})}
\newcommand\emnlpvi{EMNLP 2016 (\S\ref{sec:emnlp16})}
\newcommand\eaclvii{EACL 2017 (\S\ref{sec:eacl17})}

\newcommand\emnlpivp{EMNLP 2014 paper (\S\ref{sec:emnlp14})}
\newcommand\conllvp{CoNLL 2015 paper (\S\ref{sec:conll15})}
\newcommand\aclvp{ACL 2015 paper (\S\ref{sec:acl15})}
\newcommand\emnlpvp{EMNLP 2015 paper (\S\ref{sec:emnlp15})}
\newcommand\naaclvip{NAACL 2016 paper (\S\ref{sec:naacl16})}
\newcommand\emnlpvip{EMNLP 2016 paper (\S\ref{sec:emnlp16})}
\newcommand\eaclviip{EACL 2017 paper (\S\ref{sec:eacl17})}


\raggedbottom
\newcommand{\tofix}[1]{\hl{#1}}

% Table of contents max depth listed:
% 1 = section, 2 = subsection, 3 = subsubsection
\setcounter{tocdepth}{2}


\begin{document}

%\input{mathdefs} % my math definitions.


% UNDERLYING SPACING FOR WHOLE DOCUMENT:
% Single spacing: takes place of `draft' mode, without losing figures.
%\ssp
\hsp
% makes double-spaced: fg(for GSAS requirement, microfiche):
%\dsp


\include{frontmatter}

\chapter{Introduction}
\label{chap:introduction}
% 0. NLP tasks and the need for annotated data,
Natural Language Processing (NLP) is an active field of research which aims, broadly speaking, to teaching computers to understand human language. Achieving that goal is not easy as the computer must understand many facets of language such as syntactic, semantics, and work with different input formats such as raw text, image and speech. Most NLP algorithms employ some form of machine learning techniques. Recently, many advancements in NLP have been realized thanks to better computing resources, better understanding of the algorithms and more annotated data. 

Solving a NLP task usually involves annotating a lot of text and then applying supervised machine learning algorithms. For example, if we are interested in part-of-speech (POS) tagging, we would imagine annotating each word in the sentence with the correct POS tag such as Noun, Verb, Adjective and then training a statistical classifier. In this approach, annotated data is crucial as it provides the only guidance for the model. In recognition of the importance of annotated data, annotated resources have long been a part of NLP conferences. Moreover, the Language Resources and Evaluation Conference (LREC) is a major conference dedicated to language resources. 

% Annotated data is important but expensive/tedious/slow/ to get ... 
% - for 1000 sentence in the parsing ... 
% - time consuming and money 
% - not task dependent 
Clearly, annotated data is the ``gold standard'', however, it is expensive and time-consuming to create. Annotation projects typically require careful design, testing, and subsequent refinement of annotator guidelines, as well as quality assessment and management. For example, in the case of the Prague Dependency Treebank (PDT), it took a year to annotate the first 1000 sentences and 8 years to finish version 1~\cite{bohmovahhh:2001}. Moreover, annotated data is 
usually task specific, meaning that it cannot be reused for different purposes. In this fast-changing field, sole reliance on annotated data is risky and not a good strategy. Thus, remedying this is one focus of this thesis. 

\begin{figure}
\centering
\includegraphics[scale=0.4]{Figures/ring_plot_languages}
\caption[Fraction of world population by number of native speakers]{Fraction of world population (percentage) by number of native speakers in 2007. This diagram should be viewed in color. [sourced Wikipedia]}
\label{fig:language_by_speakers}
\end{figure}
% Low-resource languages are not uncommon 
Since it is expensive and hard to get, most annotated text is in high-resource or resource-rich languages such as English, Mandarin and Portuguese. Doing NLP is much more challenging for so-called ``low-resource" or ``resource-poor'' languages for which available resources, particularly annotated data such as treebanks, wordnets, and the like, are limited~\cite{AbneyBird2010}. Standard supervised learning techniques require significant amount of annotated text which make them unsuitable for many low-resource languages. There are approximately 7,000 languages in the world, but of these only a small fraction (20 languages) are considered high-resourced~\cite{BAUMANN14}. 
Low-resource languages are in dire need of a method to overcome the resource barrier, such that advances in NLP can be realised much more widely. Figure~\ref{fig:language_by_speakers} shows the proportion of the world's languages by number of native speakers. 
%Table \ref{tab:majorLanguageLessData} shows some major languages with no or very limited annotated data available. 
Despite the dearth of data, many languages are widely spoken such as Bengali, Punjabi, Javanese, Wu, Telugu and Vietnamese. Together, the low-resource languages shown in Figure~\ref{fig:language_by_speakers} %Table~\ref{tab:majorLanguageLessData} 
are spoken by almost 2 billion people, roughly a third of the world's population. 

% annotation to some degree 
% 3. Abundant of complementary resource... 
% - Parallel data 
% - Transfer learning ? 
Despite the lack of annotated data, even for low-resource languages there are often unannotated corpora which we can exploit in order to learn more accurate models. With the growing 
quantity of text available online, and in particular, multilingual parallel texts from sources such as multilingual websites, government documents and large archives 
of human translations of books, news, and so forth, unannotated parallel text is often more widely available. This parallel data can be exploited to bridge 
languages, and in particular, transfer information from a resource-rich language to a resource-poor language. 

Knowledge bases such as dictionaries, wordnets and 
other lexical resources are another sources of information, and exist in some form for many of the world's low-resource languages. The argument is that the 
manually annotated data is hard to obtain yet bilingual lexical resources are more widely available. For instance, the Wiktionary 
project\footnote{wiktionary.org} uses crowd-sourcing to build dictionaries in many languages using the collaborative efforts of volunteers. In this way, the 
dictionary grows in both size and language coverage over time. Panlex is another example of multilingual dictionary that covers thousands of 
languages~\cite{Kamholz14}. However, these resources are limited to only lexical items without any deeper annotation. Clues from related languages can also compensate for the lack of 
annotated data, as we expect there to be information shared between closely related languages in terms of the lexical items, morphology and syntactic structure. In 
this thesis, we investigate the method to effectively harness these additional resources alongside annotated data, aiming for a complementary effect. 

% 4. Speech part of the language 
% Languages are dying and need to preserve
% Speech is a natural way of communication, need to upderstand speech for low-resource languages
% 
Half of the world's 7,000 languages do not even have the writing system, and many are dying~\cite{lewis2009}. It is estimated that by the end of this century, 
half of the world's languages will become extinct as there are no remaining speakers~\cite{crystal2002language}. Since language captures knowledge and 
wisdom, more attention is needed for preserving these languages before they are gone forever.~\namecite{bird-EtAl:2014:W14-22} pioneered the use of speech technology to 
preserve languages using an Android application which records speech in low-resource languages and translations in a higher 
resourced language. This parallel speech translation data provides a way to preserve the language. However, it is unclear how to automatically process and learn from this collected 
data. This is one motivation for this thesis. 

%\begin{table}[h]
%  \centering
%    \begin{tabular}{lc|lc}
%    \textbf{Language} & \textbf{Speakers (M)} & \textbf{Language} & \textbf{Speakers (M)}\\
%	Bengali & 205   &   Burmese & 33 \\
%	Punjabi & 102   &  Hakka & 31 \\ 
%    Vietnamese & 90 &    Bhojpuri & 29 \\ 
%    Javanese & 85 & Tagalog & 28 \\
%    Lahnda & 82 & Yoruba & 28 \\
%    Wu & 80 &  Maithili & 27 \\
%    Marathi & 72 & Swahili & 26 \\ 
%    Tamil & 69 &    Uzbek & 26 \\ 
%    Urdu & 64 &    Sindhi & 26 \\ 
%    Gujarati & 49 &     Amharic & 25 \\ 
%    Jin & 48 &      Fula & 25 \\ 
%    Min Nan & 47 &  Oromo & 24 \\ 
%    Pashto & 39 &     Igbo & 24 \\ 
%    Kannada & 38 & Azerbaijani & 23 \\ 
%    Xiang & 38 &     Gan & 22 \\ 
%    Malayalam & 38 &   Cebuono & 21 \\ 
%    Sundanese & 38 &     Kurdish & 21 \\ 
%    Hausa & 34 &     Lao & 20 \\ 
%    Oriya & 33 &     ... & ...\\

%	\end{tabular}
%	 \caption{Major languages with little or no annotated data from \protect www.ethnologue.org (date accessed: 09/2014).}
%% CPC When citing a website, also give the date accessed.
%  \label{tab:majorLanguageLessData}%
%\end{table}	

\section{Research Questions}
Extending existing NLP methods to cater for low-resource languages is an active area of investigation. For such languages, the conventional approach using supervised machine learning are inappropriate due to the lack of annotated data. Purely unsupervised approaches appear to be a better fit. However, despite considerable effort, their performance lags well behind supervised approaches, and is rarely adequate. A more pragmatic and fruitful research direction is to exploit different sources of information aside from simple annotated text. In this thesis, we want to investigate different levels of resource requirement for low-resource languages ranging from semi-supervised and unsupervised learning, to the extreme case of processing unwritten languages. 

\noindent Thus our research questions are: 
\begin{enumerate}
%\item What is the performance gap between supervised and unsupervised approaches? Answer this question will give overview about previous approaches and the current state-of-the-art which will be the point of comparison. 
%\item What is the realistic data assumption for low-resource languages ? 
\item How can we learn more accurate models for low-resource languages using less annotated data? The assumption here is that annotated data is hard to obtain and thus we assume only a small set of gold annotations. However, other resources such as parallel data, monolingual text and bilingual dictionaries are more widely available. These resources can be used to improve performance alongside supervised approach.
\item How can we achieve more accurate models for low-resource languages without annotated data? The previous question still assumes some small amount of annotated data in the target language. However, this assumption does not always hold true, and thus this section goal is to work entirely without target annotation.
\item What can we learn from unwritten languages? This question aims at solving the extreme case where we do not even have the writing system for a low-resource language.  
\end{enumerate}
These questions are arranged in order of increasing complexity by reducing resource requirements. Generally speaking, the first and second questions are about how to improve
the performance of semi-supervised and unsupervised learning applied to low-resource languages. The third question is particularly challenging due to the complexity of audio and lack of resources in extremely low-resource unwritten languages. 

\section{Scope}
We aim to build an NLP framework for processing low-resource languages. However, even for high-resource language such as English, there is no framework to truly and completely understand English. Nevertheless, there are well-established language processing data sets for text summarization, part-of-speech tagging, coreference resolution, machine translation, named entity recognition, optical character recognition, natural language understanding, parsing, sentiment analysis, speech recognition, speech segmentation, text-to-speech, word segmentation, word sense disambiguation, question answering and natural language generation. 
% Syntac and semantic analysis is important 
Each task has different objectives and tackles different linguistic problems. However, in most case they relate to \textit{syntax} and \textit{semantics}. Some tasks such as part-of-speech tagging or parsing are purely about syntax, while word sense disambiguation is mainly about semantics. Nevertheless, tasks such as machine translation or sentiment analysis need some knowledge of both syntax and semantics. It appears that to process a language we need at least some basic tools to analyse the syntactic and semantic aspects of that language. 
% Multimodal modal ... 
The other distinguishing feature between NLP tasks is the \textit{input format}. While many tasks process raw text, optical character recognition and speech processing take image and speech as their input respectively. It is highly desirable that a framework to process a language can handle multiple input formats. Moreover, as mentioned before, many languages do not even have a writing system, and so directly processing speech is the only option. 
% 4 tasks. 

\noindent Given the time limitations of the thesis research, we focus on four NLP tasks:
\begin{enumerate}
\item Part-of-speech (POS) tagging, the task of predicting the syntax categories of lexical item, classifying words into POS categories such as noun, verb and adjective.
\item Dependency parsing, which predicts the dependency relationship between words in the sentence such as head/modifier and subject/verb. 
\item Cross-lingual word embeddings which represent lexical items from multiple languages in the same dense vector space, preserving the monolingual and bilingual properties of the languages. These embeddings form a bridge between resource-rich and resource-poor languages, allowing for transfer learning. 
\item Speech-to-text translation which learns translations between speech in a low-resource language and the textual translations in the higher-resource language. This will be useful for tasks such as keyword spotting, and also relevant for unwritten language processing.  

\end{enumerate}

% Explain why select those 4 task 
These tasks are related and approaches for the latter tasks are often based on models of the earlier ones. 
The reason for choosing these tasks is because they appear in most NLP pipelines, and an advancement in NLP cannot be realized without recourse to these tasks. We cover both syntax (task 1 and 2) and semantics (task 3 and 4), from text (task 1,2 and 3) and speech (task 4) representations of language. Also, task 4 is dedicated mainly for processing unwritten language with minimal resource requirements. We will refer to task 4 as the \emph{unwritten language processing} task hereinafter. 

\section{Contributions}
% What are the contributions ? 
% Algorithms for better performance ? 
% Dataset 
% New tasks ? NAACL submission 
% EMNLP 2014 : POS tagging mapping tagset, resolve the different across languages. 
% ACL 2015: 
% EMNLP 2015 : Dependency parsing 
We propose several algorithms motivated by transfer learning and incorporation of additional resources in a low-resource setting. We also investigate an algorithm to model unwritten language based on deep neural networks. The more detailed contributions are as follows:

\paragraph{POS tagging} We propose a semi-supervised method which incorporates noisy information from parallel text to the model as a prior. In this way, we 
demonstrate that only a small amount of annotated data is sufficient in order to achieve a large improvement in performance. The second contribution is the novel tagset mapping algorithm, applied when source and target languages employ different tagsets, hindered transfer learning.  Evaluating on the resource-poor language Malagasy, we exceed the state of the art by a large margin. This work appeared as a long paper at \emnlpiv. 

\paragraph{Dependency Parsing} We propose a semi-supervised learning algorithm based on parameter sharing in a neural network parser. The additional information we incorporate to the model concerns language relatedness. We show that a more accurate parser can be achieved using the same training data by using the reference from related model in different languages in a cascade learning setting. This work is published as a short paper at \aclv. 

We realized that we can improve the method by jointly training both models, instead of using the cascade approach. We jointly train a neural network dependency parser to model the syntax in both a source and target language. In this way, the information can flow back and forth between languages, allowing for the learning of a compatible cross-lingual syntactic representation, while also allowing the parsers to mutually correct one another's errors. This part is published as a long paper at \emnlpv. 

In the extreme case where no annotated data is available, we propose an unsupervised dependency parser, which take advantage of novel 
syntactic word embeddings. Words from both source and target languages are mapped to a shared low-dimensional
space based on their syntactic context, without recourse to parallel data.
%While prior work has struggled to efficiently incorporate word embedding information into the parsing model~\cite{mohit:ACL14,andreas-klein:2014:P14-2,chen-zhang-zhang:2014:Coling},
%we present a method for doing so using a neural network parser. 
When applied to the target language, we show consistent gains across all studied languages.
Moreover, when multiple source languages are available, we can attempt to boost
performance by choosing the best source language, or by combining
information from several source languages. %To the best
%of our knowledge, no prior work has proposed a means for selecting the
%best source language given a target language. We
%introduce two metrics which outperform the baseline of always picking
%English as the source language. We also propose a method for combining
%all available source languages which leads to substantial improvement in performance.
This work has been published as long paper at \conllv. 

\paragraph{Crosslingual Word Embeddings}
Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. 
However, previous attempts had %low-performance, 
expensive resource requirements, or difficulty incorporating monolingual data, or were unable to handle polysemy.
We address these drawbacks in our method which takes advantage of a high coverage dictionary. %in an Expectation-Maximization style training algorithm over monolingual corpora in two languages. Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and
%competitive results on the monolingual word similarity and cross-lingual document classification task. We also evaluate several methods for combining embeddings which help in both crosslingual and monolingual evaluations. 
This work has been published as a long paper at \emnlpvi. We extend our work to cover more than two languages since most prior work on building crosslingual word embeddings focuses on a pair of languages (usually involve English).
However, it is highly desirable to have a crosslingual word embeddings for many languages so that different relations can be exploited. 
%We proposed novel algorithms for post-hoc combination of multiple bilingual word embeddings, applicable to any pre-trained bilingual model. 
We extend our prior work to jointly learn multilingual word embeddings over monolingual corpora in several languages achieving uniformly excellent performance across a variety of tasks. This work has been published as a long paper at \eaclvii.

\paragraph{Unwritten language processing} For many low-resource languages, spoken language resources are more likely to be annotated with translations than 
transcriptions. This bilingual speech data can be used for word-spotting, spoken document retrieval, and even for documentation of endangered languages.
We experiment with the neural, attentional model applied to this data. 
%On phone-to-word alignment and translation re-ranking tasks, we achieve large improvements 
%relative to several baselines. On the more challenging speech-to-word alignment task, our model nearly matches GIZA++'s performance on gold transcriptions, but without recourse to transcriptions or to a lexicon.
We propose a new task, alignment of speech with text translations, including a new alignment dataset. The second contribution is extending the neural, attentional model to outperform existing models at both alignment and translation re-ranking and demonstrating the feasibility of alignment directly on source-language speech. This part has been published as a long paper at \naaclvi. 
\paragraph{In summary} our contributions are:
\begin{itemize}
\item Showing how to effectively incorporate different information (such as parallel data, or language relatedness) to the model aside from annotated data. In many case, this helps not only resource-poor languages but also resource-rich languages. 
\item Analysing and tackling many real world low-resource scenarios such as annotation mapping and limited resource such as monolingual data, bilingual dictionary and speech recording. 
\item Showing the feasibility of learning meaningful relations directly from speech data and text translation in another language. Proposing a new task of speech to text translation. 
\item Building several new datasets such as an English-Serbian bilingual lexicon induction dataset and a speech to text alignment corpus. 
\end{itemize}
\section{Thesis Overview}
%As the thesis goes, we plan to to answer the research question. 
The thesis is structured as follows. 
Chapter \ref{chap:background} lists the background needed to understand the thesis. Chapter \ref{chap:research_summary} enumerates the research outcomes 
including several publications covering all four tasks: POS tagging, dependency parsing, crosslingual word embeddings and unwritten language processing. 
Each publication is included verbatim, accompanied by an overview of research process and a retrospective view with an analysis of the strengths and weaknesses of each paper. Chapter \ref{chap:conclusion} concludes the thesis with a discussion of research questions, analysis of research outcomes and future opportunities. 
%In the appendix we will discuss other papers related to the thesis that I contributed to but shouldn't be counted toward my PhD. 


%is about POS tagging for low-resource languages which cover our EMNLP 2014 papers. Chapter 4 is about dependency parsing for low-resource languages. This chapter is mainly by publications which are our ACL 2015, CoNLL 2015 and EMNLP 2015 papers. Chapter 5 is about crosslingual word embeddings which are also by publications which covers our EMNLP 2016 and COLING 2016 papers. Chapter 6 is about speech to text alignment which is also a chapter by publication cover our NAACL 2016 paper. In each chapter, 
%\include{ch1}

\chapter{Background}
\label{chap:background}
% The importance of resources 
% Resource connect with performance 
% Resource is expensive 
% Literature is usually ignore the sarce of resource 
In this chapter, we will give an overview of low-resource natural language processing including definitions, datasets, common techniques and  high level reviews of related work. We then give background of the four tasks which will be investigated in this thesis. 

\section{Low-resource Natural Language Processing}
\subsection{Definition}
% No clear definition of what is low-resource languages.
Low-resource natural language processing has recently attracted much attention. Concretely defining what counts as a low-resource language is a research question itself.
One must consider all aspects of the language's resources, and decide the importance of each one. Moreover, it is non-trivial to come 
up with this list of all language resources and a systematic way to determine their relative importance. According to LORELEI,\footnote{Low Resource Language for Emergent Incidents (LORELEI) is a US government funded project aiming at developing 
human language technology for low-resource languages.} low-resource 
languages for which no automated human language technology exists. 
However, the term human language technology is vague. We could refine the definition by picking an essential NLP task such as syntactic parsing as the yardstick, and define a low-resource language as one that does not have any syntactically annotated corpus, which is essential 
for training a syntactic parser. Dependency treebanks are a popular form of syntactically annotated corpus. 
The Universal dependency treebank is the largest collection of dependency treebanks in multiple languages, currently 
covering 40 languages~\cite{11234/1-1699}. Thus we might consider languages outside of those 40 languages to be low-resource languages. However, with this hypothetical definition, it is questionable that can we use a single syntactic task such as 
syntactic parsing to represent language technology. Moreover, some languages (such as Buryat, Coptic, Kazakh, Sanskrit or Tamil) in those 40 languages have very modest size (fewer than 1000 annotated sentences). We expect the dependency parsers trained on those treebanks would  achieve modest performance. 

Alternatively,~\namecite{berment:tel-00006313} proposed an extensive list of basic language resources for 
measuring language resources taking into consideration the minimum set of corpora, tools and human resources. They define a language as being low-resource if the weighted score is less than a threshold. However, this definition is also heuristic, and criticized by~\namecite{prys2006blark} by lack of consideration for raw material such as newspapers. 
We take the middle ground approach by simplifying the definition of~\namecite{berment:tel-00006313}. 
We instead define low-resource languages for a particular task. 
\begin{quote}
\textit{A language is considered low-resource for a given task if there is no algorithm using currently available data to automatically do the task 
with adequate performance.}
\end{quote}
% Language specific implication 
This definition implies that a language is considered low-resource based on a specific task. 
For example, Spanish is not low resource language with respect to 
part-of-speech tagging task as the performance of a tagger is 96\% accuracy, which is clearly adequate for interpretation 
and further processing~\cite{UniversalTagSet}. However, Spanish is 
low-resource for sentiment analysis 
since  there is no annotated data. 
% Different gerer in one langauge can be consider low-resource languages
Different domains or genres in a language can also be considered low-resource. 
Taking the POS tagging task as an example, annotated corpora are mainly 
constructed for the news domain, and the accuracy of an English tagger on this domain 
can be as high as 97\%~\cite{Toutanova:2003}. However, for historical English 
these models achieve much lower accuracy of 80\%~\cite{yang-eisenstein:2016:N16-1}. In this way, 
the historical English domain counts as a  low-resource language for the POS tagging task. 
% Large scope 
With this definition, the scope of this thesis is much broader than low-density or indigenous languages. 
This is because for a given task requirement, many languages become low-resource regardless of the number of speakers or vitality.%, boosting the applicability of our approaches.  

% Low-resource today but no longer tomorrow. 
It also should be noted that a language or domain is low-resource today but might not be in the future. English Twitter text is an example. It was resource-poor 5 years ago as there were no tools and annotated datasets. However, with the high demand for social data analysis, a lot of effort has been invested into building resources and models for text normalization, POS tagging, dependency parsing and sentiment analysis which makes English Twitter text no longer a low-resource language for those tasks~\cite{han-baldwin:2011:ACL-HLT2011,gimpel-EtAl:2011:ACL-HLT2011,kong-EtAl:2014:EMNLP2014,Agarwal:2011:SAT:2021109.2021114}. 

This thesis is about improving  performance to an adequate level in a low-resource scenario. However, instead of looking at each domain (e.g. Twitter) or language individually, we investigate algorithms, which can be applied widely to many low-resource settings.   

\subsection{Language resources}
% Scale for languages given resources 
%High resource > Low resource (parallel data, some annotated data...) > Very low-resources (monolingual data...) > Remote language > Endanger language. 
Understanding the available resources is the first step in building a natural language processing framework for a target 
low-resource language. In general, we can classify languages into four categories according to their language resources. First are the \emph{High resource languages}, where there is decent-sized annotated data. This language 
category falls outside the scope of this thesis. Second are the \emph{scarce resource languages}, where there is a small quantity of an annotated data. Third are \emph{very scarce resource languages}, where no annotated data is available but there are some bilingual resources such as dictionaries or 
parallel corpora. Finally we have \emph{extremely scarce resource languages}, where the data mostly comes from field linguists. Traditionally, linguists transcribed into logbooks, and then manually digitalized them for further processing. Modern linguists document the languages using electronic devices such as voice recorders, video cameras and smart phones. However, the collected material is usually tiny and presents a non-trivial challenge for natural language processing. The majority of languages in this category do not even have a writing system, adding further complexity of phone recognition and transcription. Thus, a \textit{low-resource} or \textit{resource-poor} language is a language that either in the second, third or fourth category. By categorizing languages by resources like this, generally we can apply semi-supervised learning and unsupervised learning for the second and third category respectively. For the fourth category of extremely scarce resource language, we need a special technique which will be described in more detail in \S\ref{sec:naacl16}. This section, however, catalogues the type of resource we can reasonably expect in the real world low-resource scenario. 

\subsubsection{Field linguist annotation}
Half of the world's 7000 languages are unwritten~\cite{lewis2009}. In many cases, the only resources for some languages come from field 
linguists working on language preservation and documentation projects. There can 
be many outputs from field linguists such as a  
lexicon, or a description of the morphology and phonology. However, these are usually unsuitable for 
automated natural language processing methods because of their tiny size. 
Recently,~\namecite{bird-EtAl:2014:W14-22} proposed a new method to document a language using inexpensive mobile devices, enabling 
much faster and cheaper collaborative language documentation. The output is bilingual aligned speech between the source low-resource language and the 
target higher-resource language, also optionally including re-speaking for higher quality record and some 
transcription in the target language. In their initial experiments,~\namecite{bird-EtAl:2014:Coling} managed to collect around 10 hours of speech from indigenous 
communities in Brazil and Nepal.~\namecite{Blachon201661} used an extended version to collect more than 80 hours of speech from Congo-Brazzaville. Thus, for unwritten languages, we can expect around 100 hours of parallel speech. In our~\naaclvi\ papers, we propose a method to learn meaningful relations from parallel speech data directly without recourse to transcriptions. 

\subsubsection{Monolingual Corpora} 
Many of the other half of the world's languages have some writing system and are available online. To cheaply collect the examples of a language (monolingual data), the World Wide Web is a practical option. 
%Moreover, thank to the World Wide Web, the monolingual data for a low-resource language/domain is now easier to get. This is normally the cheapest form of resource we can get.
% Website, project where we can get resource 
The Cr\'ubad\'an project~\cite{Scannell07thecrubadan} crawled
 monolingual data for resource-poor languages; to date they have collected material in more than 2,000 languages.\footnote{crubadan.org (accessed 14/09/2016)} Wikipedia is
  another major source for monolingual data contributed by volunteers, covering more than 200 languages. The Leipzig Corpora Collection (LCC) is another project for
   collecting monolingual data, which currently covers more than 200 languages, crawled from the web~\cite{GOLDHAHN12.327.L12-1154} . 
However, we can expect very different monolingual data sizes for each language. For example, from Wikipedia, to date, 58 languages have at least 100k articles and 132 languages have at least 10k articles.\footnote{en.wikipedia.org/wiki/List_of_Wikipedias (accessed 14/09/2016)}
\namecite{GOLDHAHN12.327.L12-1154} show that in the LCC corpus in 2012, around 50 languages have at least 1 million sentences and 100 languages have at least 70k sentences. When working with languages in the top 50,  we can expect the order of a million sentences or 10 million words. Monolingual data and bilingual dictionaries are the input for our ~\emnlpvi\ and~\eaclvii\ papers, learning crosslingual word embeddings for low resource languages. 

% Why a language is low-resource 
%A language is low-resource is mostly because of social, political and financial reasons. There might be not enough funding to attract research for that language. 
%As such, not much attention is given for that language. Consequently, it lacks the research, 

%When a language is low-resource, it means that 
% What it means by low-resource 
% What is resources here 

% This definition is because resource  connect to performance .... 
% Also means that no enough funding ... to attract research on that field. 
\subsubsection{Comparable and Bilingual Corpora}
Monolingual data exhibits distributional relationships between tokens in a language. Comparable or bilingual corpora, on the other hand, relate between languages. 
As multilingual news, books, subtitles, and government websites grow, it becomes easier to get comparable corpora for many low-resource languages. They contain sets of documents in multiple languages that is topically related or ``comparable''. Wikipedia is a source for comparable data since many pages (e.g. Barack Obama) are often written in several languages. Multilingual online news services such as BBC\footnote{bbc.com are available in 32 languages  (accessed 14/09/2016) .} are another source for comparable data. STRAND~\cite{Resnik:2003:WPC:964751.964753} is the system to crawl comparable documents from the web based on the structure of the website. Their proposed system can be used to crawl comparable corpora for any language pair. %However, they could not find many language pairs and modest in size. 
%That is why for comparable corpora, probably we can get the decent size only for the
% mention STRAND 

Bilingual corpora contain sentence 
aligned translations. Bilingual corpora are particularly of interest since they are the 
main input for machine translation and can be used as the bridge between languages. 
Europarl is a popular bilingual corpus covering many European languages 
as legal documents and policies are needed to translate to all 
participating country's languages~\cite{europarl}. Opus is an open-access platform 
for bilingual corpora, with data collected from many open
 multilingual sources such as movie subtitles, the bible and European documents~\cite{TIEDEMANN12.463.L12-1246}. Opus is, probably, the
 largest collection of freely available parallel corpora, covering more than 90 
 languages, and each pair in the top 100 language pairs in Opus has more than 100 million 
 words. This is a decent size even for resource intensive NLP. However, most of the 
 top 100 language pairs are from well-supported languages with some 
 exceptions such as Romanian-Turkish and Bulgarian-Hungarian. For low-resource languages, we can not expect that much of parallel data which is usually overlooked by previous work. 
% Not realistic assumption for parallel data for many works
%Many previous approaches for low-resource NLP rely on parallel data as the bridge to 
%transfer the annotation from the source resource-rich language to the target resource-poor 
%language~\cite{Das:2011,Duongacl13}. 
In addition, due to the lack of evaluation data for low-resource languages, previous work was usually evaluated on a simulated low-resource scenario for well-supported 
languages~\cite{Das:2011}. Consequently, parallel data is much easier to get for those languages. The model that relies on large and high quality parallel data would be less applicable for low-resource languages. 

How much parallel data can we reasonably expect for a low-resource language? It is hard to 
estimate and will be different according to language. 
%However, if we are working with top 15 
%languages ($\approx100$ pairs) which mainly are European languages, we can probably get a 
%parallel corpus in order of million sentence pairs. Nevertheless, 
In our \emnlpiv\ paper, 
we experimented with two low-resource languages, Malagasy and Kinyawanda. The biggest bilingual 
corpora we could find were in the order of 100k and 10k sentences, respectively.

\subsubsection{Bilingual Dictionary}
A bilingual dictionary is a common resource for a low-resource language. %This is usually the output of linguist when the language is studied. 
Bilingual dictionaries contain word translations between a low-resource language and a major language, such as English. 
There are some notable lexical resources that collect translations for multiple languages. 
Panlex is a dictionary which currently covers around 1300 language varieties with about 12 million expressions~\cite{Kamholz14}.\footnote{An expression is usually a word in a language.} This project aims to cover all languages in the world, with the goal of 350 million expressions. 
The translations in PanLex come from various sources such as glossaries, dictionaries and automatic inferences from other languages. 
Accordingly, Panlex has high language coverage but often noisy translations.
Wiktionary\footnote{en.wiktionary.org (accessed 19/09/2016)} is another notable type of bilingual dictionary. A major part of Wiktionary 
is manually extracted from dictionaries and glossaries but also added by 
a large number of volunteers. Currently, Wiktionary covers over 2500 languages\footnote{However, more than 2000 languages have fewer than 100 expressions.} with 4.5 millions expressions. 
% Information can get from dictionary 

Bilingual dictionaries are very useful and can be used as the bridge between low-resource and higher-resource languages through bilingual translations. Aside from translations, some entries in Wiktionary and Panlex also contain additional information such as part-of-speech, pronunciation and sample sentences. 
% Size we can expect 
This raises the question of what is the likely size of a bilingual dictionary for a low-resource language? 
Table~\ref{tab:expression_wik_pan} shows the number of languages in Wiktionary and Panlex 
having minimum number of expressions, which gives the idea of what we can expect for low-resource languages. 
\begin{table}
\centering
\begin{tabular}{crr}
\toprule
Min \# Expressions & Wiktionary & Panlex \\
\midrule
2,000          & 105        & 369    \\
20,000         & 34         & 87     \\
200,000        & 9          & 23    \\
\bottomrule
\end{tabular}
\caption[Number of languages having more than minimum number of expression in Wiktionary and Panlex]{Number of languages having more than minimum number of expression in Wiktionary and Panlex. The numbers for Panlex are from~\protect\namecite{Kamholz14}.}
\label{tab:expression_wik_pan}
\end{table}

\subsubsection{Small annotated data}
Some languages have small annotated corpora for the task of interest. The size of these corpora is often inadequate for supervised learning. However, as shown in our \emnlpiv{}, \aclv{} and 
\emnlpv{} papers, with careful model design and multi-task training, even a small annotated corpus can help immensely. Again, the question of how much annotated data we can expect varies depending on the task and the languages. For example, for the part-of-speech (POS) tagging task,~\namecite{garrette:naacl13} constructed corpora of 10k words each for Kinyawanda and Malagasy as the result of a 4 hour annotation session. As for dependency parsing, the smallest corpora (in the number of words) in the universal dependency treebanks are Sankrit (1k), Kazakh (4k), Coptic (4k), Buryat (5k), and Tamil (8k) which all represent the low-resource scenario. %For low-resource languages we can not expect an annotated corpus with more than 10k annotated words. 

\subsection{Transfer learning}
Transfer learning is a common technique when working with low-resource 
languages applied in both semi-supervised and unsupervised learning setting~\cite{TackstromDPMN13,Das:2011,YarowskyAndNgai,duongIJCNLP,Hwa:2005:BPV,P14-1126}. The annotation information is transferred from the resource-rich language to the resource-poor language. In fact, most of our work in this thesis is motivated by transfer learning, covering all our publications except for \naaclvi. 
% What is transferable ? 
\subsubsection{Annotation transfer}
There are many transferable concepts between source and target languages. The most common 
form is annotation. Since annotated data is more widely available in a resource-rich language, it is transferred to a resource-poor language through bilingual resources such as bitext.
\begin{figure}
\centering
\includegraphics[scale=0.5]{Figures/LabelProjection}
\caption[Part-of-speech projection examples]{Examples of part-of-speech projection from English to German using parallel text. No tag is given to the German word \textit{klitzkie} that is not aligned.}
\label{fig:projection_example_en_de}
\end{figure}
Figure~\ref{fig:projection_example_en_de} shows an example of part-of-speech annotation projection from English to German through alignments. 
There are several successful applications of this approach to low-resource part-of-speech tagging and noun-phrase chunking~\cite{YarowskyAndNgai}, dependency parsing~\cite{Hwa:2005:BPV}, and named entity recognition~\cite{wang-che-manning:2013:ACL2013}.
The challenges for this approach are that (1) the alignment is not always accurate, (2) not all 
tokens in a target language end up annotated (e.g. German word \textit{klitzkie} in Figure~\ref{fig:projection_example_en_de}) and (3) 
the projected annotation is not always linguistically correct in the target language. For 
example, in Malagasy all numbers are considered \textit{ADJ} (adjective) which are wrongly assigned as \textit{NUM} (number) if projected from English. 
For this reason the projected annotations are usually post-processed, e.g. via rule based methods~\cite{Hwa:2005:BPV} or converted to soft-constraints~\cite{Das:2011,TackstromDPMN13} before use in supervised machine learning. 
\subsubsection{Model transfer}
% Why model transfer (accumulation of errors) ... 
The pipeline for annotation transfer normally involves the following steps: (1) train a supervised model on the source resource rich language, (2) predict annotations 
for the source language using trained model, (3) use bitext bilingual alignments to project the annotation to the target language 
(4) use the projected annotation for the target language. Each step introduces some 
errors, consequently the final target language model might be very biased. Therefore, instead 
of transferring the annotation, we can also transfer the model directly from the source 
resource-rich languages to the target resource-poor language~\cite{Zeman08cross-languageparser,P14-1126}. This is also the approach we took in the \emnlpiv, \emnlpv, \aclv\ and \conllv\ papers.

% How can we do model transfer 
The model can only be transferred when both source and target language features are in the same space. 
That is why the features that can be shared in both languages are desirable for transfer learning. Universal part-of-speech tags are an attempt to map any language-specific tagset to a universal tagset~\cite{UniversalTagSet}. The universal dependency treebank~\cite{11234/1-1699} is another attempt to 
map the annotation from different treebanks to a universal annotation. The World Atlas of 
Language Structures~\cite{wals}, which indicates structural properties of languages such as 
English having a SVO structure or Japanese being SOV, also facilitates sharing across 
languages.~\namecite{Naseem:2012:SSM} and~\namecite{tackstrom:2013:NAACL-HLT} use these 
features for transferring dependency parsers.~\namecite{Tackstrom:2012:CWC} induce crosslingual 
word clusters where lexical items in both languages are grouped using parallel data, also applied for transferring dependency parser. In this thesis, we 
investigate crosslingual word embeddings where lexicons in several languages are represented 
as dense vectors in the same semantic space, enabling transfer learning. The crosslingual word 
embeddings must capture well the monolingual and bilingual relations in the semantic and 
syntactic space. We have successfully built and applied crosslingual word embeddings for several 
tasks in our \conllv\ , \emnlpvi\ and \eaclvii\ papers. 

% How to improve the performance of model transfer 
The transferred model from the source language is normally inadequate for the target language and usually needs refinement~\cite{Zeman08cross-languageparser}.~\namecite{P14-1126} add constraints from parallel data to the transferred model.~\namecite{McDonald:2011:MTD} additionally exploit multiple source languages. We, on the other hand, take advantage of a small annotated corpora as shown in our \emnlpiv\ , \aclv\ and \emnlpv\ papers. We show that we can correct most of errors from the transferred model with guidance from small annotated data.  
\begin{table}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lm{5cm}m{6cm}}
\toprule
Paper & Topic & Resource  \\
\midrule
\namecite{kamper-etalInterspeech2015}      &  speech lexicon discovery  &  unlabelled speech \\
\namecite{Kamper:2016:UWS:2992449.2992455} & speech lexicon discovery & unlabelled speech \\ 
\namecite{Besacier:2014:ASR:2533333.2533656} & speech recognition & speech + transcription \\ 
\namecite{Khanagha:2014:PSS:2844738.2844801} & speech segmentation & unlabelled speech \\  
\midrule
\namecite{Gelling:2012:PCG:2390426.2390437} & dependency parsing and POS tagging & monolingual corpus\\
\namecite{sun-mielens-baldridge:2014:EMNLP2014} & dependency parsing &  small annotated corpus \\
\namecite{xia-lewis:2007:main} & dependency parsing & interlinear glossed text \\
\namecite{georgi-xia-lewis:2013:Short} & dependency parsing & small annotated corpus + interlinear glossed text \\
\namecite{Zeman08cross-languageparser} & dependency parsing & source language annotation \\
\namecite{tackstrom:2013:NAACL-HLT} & dependency parsing & source language annotation \\ 
\namecite{zhang-barzilay:2015:EMNLP} & dependency parsing &  source language annotation\\ 
\namecite{Naseem:2012:SSM} & dependency parsing &  source language annotation \\ 
\namecite{McDonald:2011:MTD} & dependency parsing & parallel corpus \\
\namecite{Ganchev:2009:DGI:1687878.1687931} & dependency parsing & parallel corpus \\ 
\namecite{Hwa:2005:BPV} & dependency parsing & parallel corpus \\ 
\namecite{P14-1126} & dependency parsing & parallel corpus \\ 
\midrule
\namecite{YarowskyAndNgai} & POS tagging & parallel corpus \\ 
\namecite{Duongacl13} & POS tagging & parallel corpus \\ 
\namecite{Das:2011} & POS tagging & parallel corpus \\ 
\namecite{TackstromDPMN13} & POS tagging & parallel corpus + POS dictionary\\ 
\namecite{Li:2012} & POS tagging & POS dictionary \\
\namecite{garrette:naacl13} & POS tagging & 2 hour annotation \\ 
\midrule
\namecite{DBLP:journals/corr/WangM13b} & Named Entity Recognition & parallel corpus \\ 
\namecite{darwish:2013:ACL2013} & Named Entity Recognition & parallel corpus + Wikipedia links  \\ 
\namecite{Nothman:2013:LMN:2405838.2405915} & Named Entity Recognition & Wikipedia links \\ 
\namecite{TsaiMaRo16} & Named Entity Recognition & Wikipedia links \\
\bottomrule
\end{tabular}
}
\caption{Notable related work on low-resource natural language processing.}
\label{tab:example_previous_work}
\end{table}

\subsection{Notable work}
Table~\ref{tab:example_previous_work} lists some notable published work on low-resource natural language processing covering some tasks related to speech, part-of-speech tagging, dependency parsing and named entity recognition along with the data requirements. This is by no means an exhaustive list, but gives a sense of what people have done for low-resource natural language processing and their resource assumptions.  Some prior work uses inexpensive resource such as monolingual data or unlabelled speech for which no annotation is given. However, many approaches exploit parallel corpora, glossed text\footnote{The word by word translation of a foreign language.} or speech transcriptions which are harder to obtain for many low-resource languages, which limits their applicability. Most of the papers listed in Table~\ref{tab:example_previous_work} will be discussed in more detail later in this section. 

\section{POS tagging}
We evaluate our work on four main NLP tasks applied to low-resource languages. They are POS tagging, dependency parsing, crosslingual word embedding and unwritten language processing. In this section we focus on the first task, POS tagging, which is the task of 
assigning morphological categories such as \textit{Noun}, \textit{Verb}, \textit{Adjective} to tokens in the text. POS tagging is useful in itself as an 
important step in many NLP pipelines, informing deeper layers of annotation and helping to understand the syntactic aspect of the language. We now briefly review prior 
approaches proposed for POS tagging for resource-poor languages, focusing on their supervision requirements. In our \emnlpiv\ paper, we present our own semi-supervised learning approach, which we argue has more realistic data requirements befitting the low-resource scenario. 
%\subsection{Supervised learning} 
\subsubsection{Supervised Learning}
%\begin{figure}
%\centering
%\includegraphics[scale=0.6]{Figures/learningCurveSup}
%\caption{Learning curve for 3 languages: Dutch, Italian, and Swedish. Tagging accuracy is reported for the TNT tagger evaluated on the CoNLL shared task data, as described in Section~\ref{sec:annotatedData}.}
%\label{fig:lcSup}
%\end{figure}

The traditional approach to POS tagging builds a separate tagger for each target language, usually based on supervised machine learning algorithms~\cite{TNTTagger,Brill95transformation,Toutanova:2003}. %For each language they collate a large amount of manually annotated data for training a supervised POS tagger. %The supervised style for the traditional approach has achieved very high tagging accuracy, reaching as high as 95\% accuracy for many languages \cite{UniversalTagSet}. 
%The main challenge for POS tagging lies in the lack of training data. 
Supervised learning needs manually annotated data which is time consuming and costly to construct. If we were to apply supervised learning to a resource-poor language 
%TODO: put the table listing some state-of-the-art POS tagger.
the first question we would have to consider is the amount of annotated data needed. This is a difficult question to answer in general, due to the lexical and syntactic properties of the language, %it's common knowledge that it is usually harder to learn POS tagger for morphology rich language which denoted in bigger tagset size, 
 as well as the cost of manual annotation. 
% CPC How does the cost of annotation affect this?
%Figure~\ref{fig:lcSup} shows the learning curve for 3 languages, illustrating relatively high accuracy when trained on a corpus of 50k tokens, however accuracy diminishes for smaller training samples, for example, 1k tokens results in an absolute drop of roughly 20 percent.
%It's common knowledge that it is usually harder to learn POS tagger for morphology rich language which denoted in bigger tagset size. %The annotated data is from CoNLL shared tasks on dependency parsing~\cite{buchholz-marsi-2006}. 
%We use TNT tagger~\citep{TNTTagger} which is an implementation of second-order Hidden Markov Model. We use TNT because of its speed and close to the state-of-the-art performance. 
%Dutch, Italian and Swedish have 12, 30 and 41 tags respectively. At 50k tokens, Dutch achieves best performance, continue by Italian and Swedish. This confirm our intuition that bigger tagset are usually harder to learn. 

%POS tagging information are usually used within other applications. Each context yields a different performance requirement. For example, dependency parsing directly use POS information. A single POS error might lead to fail dependency parsing tree. Thus, we might need %POS tagging accuracy as high as 95\% or 97\% requiring 
%much more than just 50k tokens. %On the other hand, other tasks such as noun-phrase chunking is less severely affected by POS tagging accuracy. In that case, probably $\sim$90\% accuracy which requires around 30k annotated tokens to train on, is acceptable. 
Moreover, corpus annotation is time consuming and costly. For example, for the POS layer of the Penn Treebank~\cite{PenTreeBank} it took 3 years to annotate 4.5 million tokens. For now, we cannot expect anywhere near as a large annotated corpora for resource-poor languages. However, \namecite{garrette:acl13} show that noisy POS annotations for 1,000 tokens are easy to acquire with around 1 hour of manual effort. This raises the challenge of how we can get more data and make the best make use of such tiny amounts of annotated data. %Naively using supervised learning on this sized data results in low accuracy, as illustrated in Figure~\ref{fig:lcSup}, and in this paper we developing approaches to improve accuracy by combine this form of limited supervision with an unsupervised projection approach. 
%Moreover, figure~\ref{fig:lcSup} also shows that Italian and Swedish POS performance are actually converging. Adding more data are not likely to boost the performance. We should find another source of information aside from supervised data to aid the learning process. This will be discussed in more detail in Section~\ref{sec:semisup}. 

\subsubsection{Unsupervised learning}
Unsupervised approaches are well suited for resource-poor languages since they do not require manually annotated data, and unlabelled data is relatively easy to acquire. These approaches try to group words having the same morphosyntactic properties into clusters~\cite{Christodoulopoulos:2010,unSupPOSClustering,chineseWhisper}. It is believed that words in the same cluster are likely to have the same POS tag. One problem with this approach is determining the number of clusters. Defining that number beforehand might not be a good solution~\cite{unSupPOSClustering}. We might force the algorithm to separate coherent clusters or to join unrelated ones. On the other hand, letting the algorithm choose when to stop could result in clusters that are either too specific or too general. Evaluation is another major consideration, since we often do not have gold data to compare with. %Evaluation is also another major consideration. Normally, clustering algorithms are evaluated based on the perplexity (or entropy) of the cluster~\citep{Christodoulopoulos:2010}. In the case of tagging, we are expecting that all words in the same cluster have the same tag. Therefore, the lower the perplexity, the better. However, is it what we are looking for? The answer is no, we want to compare with gold-standard test data to know the tagging performance. \citep{ManyToOneEvaluate} suggested a \textit{many-to-one} evaluation. The induced tag for each cluster is the most frequent tag of the items in the cluster, consulting the gold-standard data. However, there can be cases where two clusters have the same tag. To resolve this issue,\textit{ one-to-one} evaluation puts the restriction that each gold tag corresponds to one cluster only. Normally, this is done by greedy matching, which aims at maximizing  accuracy. Nevertheless, the number of clusters and gold tags are likely to be different. In that case, some clusters or gold tags will not be matched. However, both \textit{many-to-one} and \textit{one-to-one} evaluation schemes require gold-standard data to find the most appropriate tag for each cluster. This is a chicken-and-egg problem since if we have gold-standard data then we do not need to take an unsupervised approach. Besides, we can also use some heuristic method to determine the tag for each cluster such as cluster size (e.g. the biggest cluster is Noun). 
%Unsupervised approach is typically suitable for resource-poor language since it doesn't need any manually annotated data and unlabelled data is relatively easy to acquire. 
Moreover, the biggest problem suffered by the unsupervised approach is the much worse performance relative to supervised methods, which hinders its utility in real world applications~\cite{Christodoulopoulos:2010,Blunsom:2011}. %Some unsupervised POS tagger might still achieve quite high performance  using tag dictionary \cite{Goldberg08emcan,Das:2011}. However, tag dictionary is also a huge form of supervision and very expensive to acquire. 

\subsubsection{Semi-supervised learning}
As mentioned above, supervised learning needs large training corpora, 
which are only available for resource-rich languages. Unsupervised POS tagging, on the other hand, is suitable for resource-poor languages since it requires only unannotated text, yet its relatively poor performance means that it is not suitable for practical applications. 
Semi-supervised learning appears to a better fit which is also the approach in our \emnlpiv\ paper. We show that we can create a high performance POS tagger exploiting only a tiny amount of annotated data and some distant supervision from additional resources. It is important to understand what kind of supervision signal we might have from additional resources which will be reviewed in the following section. 
%However, to further improve the performance, we also incorporate different 
%It is desirable to find a different form of supervision to compensate for the lack of annotated data which will be reviewed below. 
%---------------------

\subsection{Typologically related information}

For closely related languages, such as dialects of the same language or those in the same language family, the lexicon and syntactic structures of the languages are likely to be highly similar. These kinds of similarities can be exploited when developing tagging models for low-resource languages \cite{Hana04,Feldman06,reddy2011crosspos}. They propose using transition probabilities directly from a high-resource language and estimating the emission probabilities separately\footnote{The emission probability is usually estimated from small annotated data in the target language.} for a target language Hidden Markov Model tagger. Note that this method does not need parallel data, as no alignments are required, however monolingual annotated data is required for related languages, which may not available for many low-resource languages.\footnote{Especially for languages only spoken by small communities, in which case the best we might hope for is parallel data between the target language and a mainstream `contact' language, such as English or a pidgin.}  

\subsection{Projected annotation}

\namecite{YarowskyAndNgai} pioneer the use of parallel data for projecting tag information from a resource-rich language to a resource-poor language. They first tag the source resource-rich language using a supervised POS tagger, and the tagging is then projected to the target resource-poor language through word alignment. They observe that although this works well in many cases, the projected tags are very noisy. Thus, they apply a heuristic based on the sentence alignment score to filter out noisy alignments. Finally, the projected tags are used to build the target language tagger, which can then be applied to other texts. Our previous work use a similar method, using sentence alignment scores to rank the goodness of sentences~\cite{Duongacl13}\footnote{This work is done before my PhD and should not be counted.}. We train a seed model from a small part of the projected data, then apply this model to the rest of the data using self-training with revision.

%SB: the following paragraph is hard to follow
\namecite{Das:2011} also use parallel data but additionally exploit graph-based label propagation to expand the coverage of labelled tokens. Each node in the graph represents a trigram in the target language. Each edge connects two nodes which have similar context. Originally, only some nodes receive a label from direct label projection, and then labels are propagated to the rest of the graph. Rather than use the labels directly, \namecite{Das:2011} instead use the labels to extract a tag dictionary which is used as constraints in learning a feature-based HMM \cite{featurebaseHMM}. Both \namecite{Duongacl13} and \namecite{Das:2011} achieve 83.4\% accuracy on a test set of 8 European languages (Table~\ref{tab:taggingAccPrevModels}).

\begin{table*}
\tabcolsep 3pt
\begin{center}
\begin{tabular}{lccccccccc|c}
\toprule
        ~ & da & nl & de & el & it & pt & es & sv & & Average \\
\midrule
\namecite{Duongacl13} & 85.6 & 84.0 & 85.4 & 80.4 & 81.4 & 86.3 & 83.3 & 81.0 & & 83.4 \\      
\namecite{Das:2011} & 83.2 & 79.5 & 82.8 & 82.5 & 86.8 & 87.9 & 84.2 & 80.5 & & 83.4 \\
\namecite{Li:2012} & 83.3 & 86.3 & 85.4 & 79.2 & 86.5 & 84.5 & 86.4 & 86.1 & & 84.8 \\
\namecite{TackstromDPMN13} & 88.2 & 85.9 & 90.5 & 89.5 & 89.3 & 91.0 & 87.1 & 88.9 & & 88.8 \\
\bottomrule
\end{tabular}
\caption[Previously published token-level POS tagging accuracy]{Previously published token-level POS tagging accuracy for various models across 8 languages: Danish (da), Dutch (nl), German (ge), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish (sv) evaluated on CoNLL data using universal POS tagset.% as described in Section~\ref{sec:annotatedData}. %~\cite{buchholz-marsi-2006}.%The best results on each language, and on average, are shown in bold
 }
\label{tab:taggingAccPrevModels}%
\end{center}
\end{table*}

\subsection{Dictionary Information}
A tag dictionary specifies the set of allowable tags for a word. Even an incomplete or noisy tag dictionary is sufficient to allow a POS tagger to be learned using standard unsupervised inference, such as the Expectation Maximization (EM) algorithm, where the entries in the tag dictionary constrain the tags for each word~\cite{Kupiec1992225,Merialdo:1994,Banko:2004,Goldberg08emcan}. The usefulness of tag dictionaries comes from the fact that many words have very few possible tags and thus the tag dictionary drastically restricts the search space, while also steering EM away from poor solutions.
%% LONGDT : Need to put the citation here. I remember reading somewhere said that 90% of the words in French corpus have less than 2 possible tags.
With a dictionary derived from gold-standard data,~\namecite{Das:2011} achieved an accuracy of approximately 94\% on the same 8 languages. The effectiveness of a gold-standard dictionary is undeniable, however it is costly to build one, especially for resource-poor languages. Cheaper crowd-sourced dictionaries are also valuable, as demonstrated by \namecite{Li:2012} who used Wiktionary to achieve 84.8\% accuracy on the same 8 languages (see Table~\ref{tab:taggingAccPrevModels}). Note, however, that there are large differences in  performance for words appearing in dictionary versus out-of-vocabulary (OOV) words (89\% vs  63\%), which suggests that their approach will be less suitable for small and incomplete POS dictionaries. 

\namecite{TackstromDPMN13} combined both token information from bilingual projection and type 
constraints from Wiktionary to achieve the current state-of-the-art in low-resource tagging. 
Their approach first builds a tag lattice, which is then pruned using the token information 
and type constraints. The remaining paths are used to train a Conditional Random Field (CRF) 
tagger. They achieved 88.8\% accuracy on the same 8 languages (see 
Table~\ref{tab:taggingAccPrevModels}). In our \emnlpiv\ paper, we mainly compare the 
results of our approach with~\namecite{TackstromDPMN13}. Note that our method and theirs have 
very different data requirements: we use a small corpus of annotated part-of-speech in the 
target language, but only limited parallel data and no tag dictionaries, while they use orders 
of magnitude more parallel data as well as implicit supervision, courtesy of their tag 
dictionary. As argued above, while both approaches have limited supervision, our data 
requirements are more appropriate to a low-resource scenario. 

Table \ref{tab:taggingAccPrevModels} summarises the performance of related work across all 8 languages. Note that these methods vary in their reliance on external resources. The systems listed in Table~\ref{tab:taggingAccPrevModels} are sorted in the ascending order of resource usage.~\namecite{Duongacl13} use the least, i.e.\ only the Europarl Corpus~\cite{europarl}.~\namecite{Das:2011} additionally use the United Nations Parallel Corpus.~\namecite{Li:2012} did not use any parallel text but used Wiktionary.~\namecite{TackstromDPMN13} exploited most parallel data by additionally using parallel data crawled from the Web, as well as 
%\fixme{We don't have the specific statistic of how much they use}
using the tag dictionary from~\namecite{Li:2012}. The pattern of results in Table~\ref{tab:taggingAccPrevModels} illustrates a common lesson in NLP: when adding additional resources, the models perform better.   
 
\subsection{Small Annotated Data Information}

An alternative approach for tagging resource-poor languages is to assume a small corpus of manually annotated data. \namecite{garrette:acl13} built a POS tagger for two resource-poor languages, Kinyarwanda and Malagasy. They used no parallel data, but instead exploited four hours of manual annotation by native students to label 4,000 tokens or 3,000 word-types. These tokens or word-types were used to build a tag dictionary. They employed label propagation to expand the coverage of this dictionary, much like \namecite{Das:2011}. The dictionary was used to label training examples, from which they trained a tagger. This achieved 81.9\% and 81.2\% accuracy for Kinyarwanda and Malagasy respectively. %Note that although they claim to use only 4 hours of annotation, their use of an external tag dictionary compromises this claim, and consequently limits the portability of their approach to other low-resource settings which do not have existing dictionaries.
 
The method we propose in our \emnlpiv\ paper is similar in that we also use a small amount of annotation. However, we directly use the annotated data to train the model rather than indirectly via a tag dictionary. We argue that with a proper ``guide'', namely parallel projection, we can take advantage of very limited annotated data. Our approach is also able to use a dictionary, although even without this form of supervision our method results in high accuracy taggers, well above baseline approaches and in most cases outperforming the previous state of the art.  
% tac -- I added the last sentence. Feel free to cut if you this it's too much.
% LDC -- I think it's fine 
%---------------------
 
% This should be the conclusion for this chapter 
%In our EMNLP 2014 paper, we propose a semi-supervised method to narrow the gap between supervised and unsupervised 
% approaches. We demonstrate that even a small amount of supervised data leads to substantial improvement. 
% Aside from this small amount of annotated data, the supervision signal also comes from parallel data of 
% the resource-rich (source) and resource-poor (target) languages. The parallel data provides a 
% bridge that enables us to transfer POS information from a resource-rich to a resource-poor language through the word alignment. 

%While annotated text tends to be expensive to acquire, parallel data between the resource-rich and resource-poor languages, on the other hand, is relatively easier to acquire in many cases, thanks to the development of multilingual documents from government projects, book translations, multilingual websites, and so forth. Moreover, this approach also exploits the idea that tag ambiguity in one language is disambiguated through different alignments. Consider the example, \textit{Buffalo buffalo buffalo buffalo},\footnote{The bison of Buffalo (a city in the United States) bully (other) bison.} and its Vietnamese translation \textit{Trau o Buffalo bat nat trau khac}, as in Figure \ref{fig:EnViparallel}. The ambiguous usages of \textit{buffalo} have different translations: \textit{trau} (common noun - \textit{NNS}), \textit{bat nat} (verb - \textit{VB}), \textit{Buffalo} (proper noun - \textit{NNP}). Thus, the different translations help to disambiguate the POS tag of the word \textit{buffalo}.
%\begin{figure}
%\centering
%\includegraphics[scale=0.5]{Figures/Buffalo_buffalo}
%\caption{Sample of English--Vietnamese parallel data. The usages of the ambiguous English word \emph{buffalo} are disambiguated through their alignment with different Vietnamese words.}
%\label{fig:EnViparallel}
%\end{figure}

%\section{Background and Related Work}
%\label{sec:semisup}
%As mentioned above, semi-supervised or distantly-supervised methods are well suited to resource-poor languages. In this section, we review forms of supervision used in semi-supervised methods for low-resource POS tagging in the literature. 
% PRT shouldn't be include in the Universal tagset
\begin{table}[t]
\centering
\tabcolsep 3pt
\begin{tabular}{crcl}
\toprule
Lang & Size (k) & \# Tags & Not Matched \\
\midrule
da & 94 & 8 & DET, PRT, PUNC, NUM \\
nl & 203 & 11 & PRT \\
de & 712 & 12 & \\
el & 70 & 12 & \\
it & 76 & 11 & PRT \\
pt & 207 & 11 & PRT \\
es & 89 & 11 & PRT \\
sv & 191 & 11 & DET \\
\midrule
kin & 9.3 & 9 & PRT, PRON, NUM\\
mlg & 9.5 & 11 & NUM \\
\bottomrule
\end{tabular}
\caption[The size of annotated data, number of tags included and missing for all considered languages]{The size of annotated data, number of tags included and missing for the 8 European languages: Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es) Swedish (sv) and 2 resource-poor languages Kinyarwanda (\texttt{Kin}) and Malagasy (\texttt{Mlg}).} 
\label{tab:tagMissing}
\end{table}

%<HERE >>>>>>>>>>>>>>>>>


\subsection{Universal tagset}
\label{sec:universalTagset}
Core to many of the projection methods described above is an assumption of a matching tagset between the source and target languages. This way, labels projected from the source have meaning in the target language, and can be used directly as target labels or constraints. However, it is uncommon for languages to have been annotated with same tagset. For this reason, these approaches use the Universal POS Tagset~\cite{UniversalTagSet}. This tagset consists of a list of tags that are said to be shared across languages, as well as mappings into this scheme from native tagsets in several languages. The universal tagset is extremely useful in multilingual applications, enabling joint multi-lingual modelling as well as simpler evaluation of results across languages. 
In a low-resource setting, using the universal tagset can simplify the problem of matching between different tagsets. 
For low-resource languages without an official tagset, such as Bengali or Lahnda, the Universal Tagset would be a good starting point for linguistic annotation.

The Universal Tagset from~\namecite{UniversalTagSet} consists of 12 common tags: \textit{NOUN, VERB, ADJ} (adjective), \textit{ADV} (adverb), \textit{PRON} (pronoun), \textit{DET} (determiner and article), \textit{ADP} (preposition and post-position), \textit{NUM} (numerical), \textit{CONJ} (conjunctions), \textit{PRT} (particle), \textit{PUNC} (punctuation) and \textit{X} (all other categories including foreign words and abbreviations). \namecite{UniversalTagSet} provide the mapping from several language-specific tagsets to the universal tagset.

Nevertheless, using the Universal Tagset looses information, such as tense and case information is often lost in the mapping. For example, the Penn treebank tags verbal tags \textit{VB, VBD, VBG, VBN, VBP, VBZ} are mapped to the generic \textit{VERB} tag in the Universal tagset. Moreover, the mapping is not always straightforward. Table \ref{tab:tagMissing} shows the size of the annotated data for each language, the number of tags presented in the data, and the tags that are not matched. We can see that only 8 tags are presented in the annotated data for Danish, i.e, 4 tags (\textit{DET, PRT, PUNC,} and \textit{NUM}) are missing.\footnote{Many of these are mistakes in the mapping, however, they are indicative of the kinds of issues expected in low-resource languages.}
%LongDT : why it's missing, because of mapping or linguistic reason? Could you check this for me Steven ?
%Therefore, when we project the labels from a resource-rich language (i.e, English) to Danish through alignment, we won't get any credit from tokens tags as either \textit{DET, PRT, PUNC} or \textit{NUM}.
Thus, a classifier using all 12 tags will be heavily penalized in the evaluation.

\namecite{Li:2012} considered this problem and tried to manually modify the Danish mappings i.e. map tag \textit{AC} and \textit{AO} as \textit{NUM} or match tag \textit{U} to \textit{PRT} etc. Moreover, \textit{PRT} is not really a universal tag since it only appears in 3 out of the 8 languages.~\namecite{plank-hovy-sogaard} state that \textit{PRT} often gets confused with \textit{ADP} even in English. We will later show that the mapping problem causes substantial degradation in the performance of a POS tagger exploiting parallel data. The method presented in our \emnlpiv\ paper is more target-language oriented: our model is trained on the target language, in this way, only relevant information from the source language is retained. Thus, we automatically correct the mapping, and other incompatibilities arising from incorrect alignments and syntactic divergence between the source and target languages. 
 
\section{Dependency Parsing}

\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
\Tree [.S [.NP [.PRP I ] ].NP [.VP [.VBP like ] [.NP [.DT a ] [.JJ big ] [.NN meal ] ].NP ].VP ].S
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
\begin{dependency}[theme = simple]
   \begin{deptext}[column sep=1em]
      I \& like \& a \& big \& meal \\
   \end{deptext}
   \deproot{2}{ROOT}
   \depedge{2}{1}{NSUBJ}
   \depedge[arc angle=90]{2}{5}{DOBJ}
   \depedge[edge start x offset=0pt, arc angle=80]{5}{3}{DET}
   \depedge[edge start x offset=-7pt, arc angle=30]{5}{4}{AMOD}
\end{dependency}
\end{minipage}
\caption{Phrase structure tree (Left) and Dependency tree (Right) of the same sentence.}
\label{fig:parseSample}

\end{figure}


%%% VIETNAMESE EXAMPLES 

%\begin{dependency}[theme = simple]
%   \begin{deptext}[column sep=1em]
%      Manh \& dat \& cua \& dan \& bom \& khong \& con \& nguoi \& ngheo \\
%   \end{deptext}
%   \deproot{7}{ROOT}
%   \depedge{7}{1}{NSUBJ}
%   \depedge[edge start x offset=-7pt, arc angle=90]{7}{6}{NEG}
%   \depedge[edge start x offset=7pt, arc angle=80]{1}{2}{AMOD}
%   \depedge{1}{3}{DET}
%   \depedge{3}{4}{POBJ}
%   \depedge{4}{5}{NN}
%   \depedge{7}{8}{DOBJ}
%   \depedge{8}{9}{AMOD}
%   \depedge[edge start x offset=0pt, arc angle=80]{7}{3}{DET}
%   \depedge[edge start x offset=-7pt, arc angle=30]{5}{4}{AMOD}
%\end{dependency}


%\begin{figure}
%\centering
%\includegraphics[scale=0.5]{Figures/phraseStructure}
%\;\;\;\;\;\;
%\includegraphics[scale=0.6]{Figures/dependencyParse}
%\caption{Phrase structure tree (Left) and Dependency tree (Right) of the same sentence}
%\label{fig:parseSample}
%\end{figure}
While part-of-speech tagging  operates on the word level by providing information about the syntactic category of each word in the sentence, we now move to sentence level parsing as our second task. Parsing is the task of predicting the underlying structure of a sentence. There are two main structured representations: (1)~phrase structure, or (2)~dependency tree. Phrase-structure trees represent nesting structure of phrases such as noun phrase, verb phrase, preposition phrase etc. Dependency trees, on the other hand, show the dependencies between words such as modifier, subject-verb. For example, the sentence ``\textit{I like a big meal}" has two representations as shown in Figure \ref{fig:parseSample}.

Phrase structure trees show the grammatical structures of the sentence. However, for each language, the phrase structure might be very different. For example, English favours \textit{``Subject Verb Object"} structure while Japanese switches the position of \textit{Object} and \textit{Verb} (putting the main verb at the end of the sentence). Thus, when transferring information from the source language to the target language, a phrase structure tree is not particularly suitable as each language requires very different tree shapes. Dependency trees, in contrast, show the semantic structure, i.e. answering the question \textit{who did what to whom by which means?} Thus, dependency structures will be more similar across languages. In addition, dependency trees are better at capturing long distance relations, which are desirable in many applications.  
%Todo : list of advantages of dependency structure 
We will use this structure for building parsers for target resource-poor languages.

A dependency tree is usually formalized as labelled directed graph $\text{G} = (\text{V},\text{A})$ where $\text{V}$ is the set of nodes, $\text{A}$ is the set of arcs. For example, the dependency tree in Figure~\ref{fig:parseSample} has 
\begin{align}
\text{V} &= \{I, like, a, big, meal\}\\
\text{A} &= \{(like, \texttt{nsubj}, I), (like, \texttt{dobj}, meal), (meal, \texttt{det}, a), (meal, \texttt{amod}, big)\} 
\end{align}
$\text{A}$ is the set of triples $(w_i,\texttt{r},w_j)$ which represents the relation $\texttt{r}$ from the head $w_i$ to child $w_j$. %~\namecite{Kubler:2009:DP:1538443} suggested some relations between head and dependent: 
%\begin{itemize}
%\item  $w_i$ is compulsory but $w_j$ is optional 
%\item $w_i$ select $w_j$ and determine the roles of $w_j$ whether obligatory or optional
%\item  The form of $w_j$ depends on $w_i$ with respect to agreement and government. 
%\item The linear position of $w_j$ is determined according to $w_i$
%\end{itemize}
Most of the time the roles of head and child are distinguishable. However, sometimes it is hard to differentiate, especially when it involves articles, complementizers and auxiliary verbs. For example, in the sentence \textit{I hand in my thesis}, it is unclear whether $hand$ is the head of $in$ or vice versa. Thus, dependency parsing is a much harder task compared with POS tagging, especially in the low-resource scenario. 

The rest of this section is organized as follows. In section~\ref{sec:monolingualDep}, we will review some supervised methods for building a dependency parser. In section~\ref{sec:crosslingualDepParsing}, we review crosslingual methods applied to resource-poor languages. 

\subsection{Supervised dependency parsing}
\label{sec:monolingualDep}
\subsubsection{Grammar-based approach}
Phrase structure trees have a long history with many algorithms developed for phrase structure parsing. Naturally, people have attempted to apply phrase structure parsing algorithms to dependency tree parsing. One notable approach uses a context free dependency grammar in a similar way to context free grammar of phrase structure parsing~\cite{Nivre_twomodels,eisner-blatz-2007,MarkP07-1022}.
However, in a context free dependency grammar, all non-terminal nodes (e.g. \textit{S, NP, VP}) are replaced with an actual word. This is the simplest way to convert from phrase structure grammar to dependency grammar. After the dependency grammar is constructed, we can directly use any phrase structure parsing algorithm such as CKY~\cite{Younger1967189}. One disadvantage of using dependency grammar is that it is very hard to capture long distance relations.  

Another adaptation for dependency parsing uses tree transformation rules. That is, the sentence is parsed using a phrase structure parser. Phrases are converted to dependency relations using manually crafted head-rules~\cite{Marneffe06generatingtyped,Yamada03statisticaldependency}. However, these rules are language specific and need a lot of expertise to build. 

\subsubsection{Transition-based approach}
Transition-based parsing is more recently developed~\cite{Nivre:2008:ADI}. It is similar to a finite state automaton which consists of sets of \textit{configurations} and \textit{transitions}. The parsing algorithm will choose a list of transitions that transform the initial configuration to the terminal configuration. 
\paragraph{Configuration}
Given a sentence of tokens $w = w_0,w_1, ... w_n$, where $w_0$ is set to be the dummy $ROOT$ token, a configuration is defined as a triple $c=(S,Q,A) $ where 
\begin{itemize}
\item $S$ is the stack of partially processed words; 
\item $Q$ is the queue of remaining words; 
\item $A$ is the set of arcs that form the partially parsed tree. 
\end{itemize}
Each configuration $c$ aims at capturing a partial analysis of a sentence. The initial configuration for the above sentence $w$ is defined as 
$$c_{init}  = (S_0,Q_0,A_0)$$ 
Where $S_0 = [w_0]$ contains only the dummy $ROOT$. $Q_0 = [w_1,w_2,...w_n]$ contains all the remaining words and $A_0 = [\texttt{\O}]$. The terminal configuration is defined as 
$$c_{terminal} = (S_{ter}, Q_{ter}, A_{ter}) $$ 
Where $Q_{ter} = [\texttt{\O}]$ for any $S_{ter}$ and $A_{ter}$. That is, the algorithm terminates when there are no words remaining in the queue. 
% To Do : Give an example here 

\paragraph{Transition}
The set of transitions are used to transform the initial configuration $c_{init}$ to the terminal configuration $c_{terminal}$. There are 3 transitions:
\begin{itemize}
\item \texttt{left-arc}($r$)\;\; : $(S|w_i,\; w_j|Q,\; A) \Rightarrow (S,\;w_j|Q,\; A\cup \{(w_j,r,w_i)\})$
\item \texttt{right-arc}$(r)$ : $(S|w_i,\; w_j|Q,\; A) \Rightarrow (S,\;w_i|Q,\; A\cup \{(w_i,r,w_j)\})$
\item \texttt{shift}$ \;\;\;\;\;\;\;\;\;\;\;\;\;\;: (S,\; w_i|Q,\; A) \;\;\;\;\;\Rightarrow (S|w_i,\;Q,\; A)$
\end{itemize}

% Todo : Add the description for each transition 
The \texttt{left-arc}($r$) transition for dependency relation $r$ with $w_i$ at the top of stack $S$ and $w_j$ at the first position of queue $Q$, adds a dependency arc $(w_j,r,w_i)$ to $A$ and pops the stack. The \texttt{right-arc}($r$) transition for dependency relation $r$ adds the dependency $(w_i,r,w_j)$ to $A$, pops the stack and replaces the first element of the queue with $w_i$. \texttt{shift} simply removes the first word of the queue and pushes it at the top of stack. 
A pre-condition for \texttt{left-arc} is that both the stack and the queue are non-empty and $i\neq0$. 
\texttt{right-arc} requires that both stack and queue are non-empty. The precondition for \texttt{shift} is that the buffer is non-empty. 

\paragraph{Parsing Algorithm}
The parsing algorithm decides what transition is applied to a given configuration.~\namecite{Nivre:2008:ADI} formalized it as a supervised classification task. The dependency treebank is converted to the training data using some heuristic rules.\footnote{This training data can be generated dynamically as in~\namecite{C12-1059}.} The classifier is trained on this data set. Crucially, the classifier should give a ranking for each predicted transition.
If the first prediction is not applicable (i.e. the pre-condition is not satisfied), the next one will be considered. 
% To do: proof that algorithm always terminate. 
%Importantly, the algorithm always terminates in $O(n)$ steps. Each \texttt{left-arc} or \texttt{right-arc} reduces the stack size by 1 and \texttt{shift} increases the stack size by 1. There are maximum $n$ \texttt{shift} operations since each \texttt{shift} also reduces the queue size by 1. Therefore, maximum number of \texttt{left-arc} and \texttt{right-arc} is also $n$. Therefore, maximum number of transitions is $2\times n$. Moreover, the parsing algorithm can always pick a valid transition for each configuration because \texttt{shift} is always valid (except if the current configuration is the terminal configuration). 

%Todo : talk more about feature set and algorithms 
% Todo : more accurate system can be achieve by not using static rules 
There are many variations of transition based parsing. \namecite{Nivre:2009:NDP:1687878} introduced the \texttt{swap} transition to deal with non-projective dependency parsing where there are crossing edges w.r.t. the linear word order.~\namecite{chen-manning:2014:EMNLP2014} exploited a neural network based classifier for the parsing algorithm instead of the original support vector machine classifier which we extended for our \aclv, \emnlpv\ and \conllv\ papers. 

\subsubsection{Graph-based approach }
The graph based approach formalizes dependency parsing task as finding the maximum spanning tree on the weighted fully connected graph~\cite{McDonald:2005:NDP}. The graph $G = (V,E)$ for a sentence $w = w_0,w_1,w_2....w_n$ is constructed as follows. 
\begin{itemize}
\item $V$ = {$w_0,w_1,...w_n$}
\item $E$ = {$(w_i,\alpha_{ij},w_j)$} for every $i,j$ where $\alpha_{ij}$ is the weight. 
\end{itemize}
The~\namecite{1264361} algorithm is used to find the maximum spanning tree. First, all vertexes are selected. Incoming edges with highest weight are added to the graph one by one. If the resulting graph is a tree then it is the maximum spanning tree. If it is not, after the cycle is collapsed into a single node, the weights are updated and the algorithm is repeated. A remaining question is how to estimate the weight of each edge.~\namecite{McDonald:2005:NDP} applied the Margin Infused Relaxed Algorithm (MIRA) for estimating the weights trained on the treebank. 

The graph-based approach is the natural solution for non-projective dependency parsing since it makes no assumption about word order. Unlike the transition-based approach, the graph-based approach involves exact inference. Therefore, the running time is much slower. However, the graph-based approach is more robust. The transition-based approach is prone to errors, e.g. errors in early states might accumulate and led to a model bias. An interesting observation is that errors made by the graph-based approach and the transition-based approach are very different and mostly no overlapping.~\namecite{Nivre08integratinggraphbased} and \namecite{Zhang:2008:TTP} proposed methods to combine the strength of both approaches in a hybrid approach. 
%Todo : talk more about hybrid approach here 

%\subsection{ConLL shared task on dependency parsing}
%The ConLL 2006 and 2007 organized the shared task on dependency parsing for various languages. 
%\subsection{Evaluation}
\subsubsection{Evaluation}
The common evaluation metric for dependency parsing is attachment score which is the percentage of words having the correct head thanks to the single-head property of the dependency tree. There are two versions of attachment score which are unlabelled attachment score (UAS) and labelled attachment score (LAS)~\cite{kubler2009dependency}. The first one only looks at the head while the second metric additionally looks at dependency labels. 
%- UAS vs LAS 
%- Total sentence score 
%- Ignore punctuation (Interesting story here). Use malt eval or official web.
%- Limit evaluation to 10 words or less 

\subsubsection{Universal Treebank}
As with universal POS tagset, it is highly desirable to have universal annotation for 
dependency treebanks.~\namecite{DBLP:conf/lrec/ZemanMPRSZH12} pioneered on building an unified 
annotation, named HAMLED for treebank in multiple languages. They proposed a mapping to transform 
each language specific treebank to the Prague Dependency Treebank 
style~\cite{bohmovahhh:2001}. 
~\namecite{mcdonald-EtAl:2013:Short} took a different approach and built the Google 
Universal Treebank for many languages using the Stanford dependency 
style~\cite{deMarneffe:2008:STD:1608858.1608859} and the universal POS 
tagset~\cite{UniversalTagSet}.~\namecite{ROSA14.915} 
extended~\namecite{DBLP:conf/lrec/ZemanMPRSZH12} to build HAMLED 2.0 which covers more than 30 
languages using similar annotation with Google Universal Treebank. 
As an effort to better accommodate language differences and unify prior work, 
\namecite{11234/1-1699} proposed the Universal Dependency Treebank, employing the Stanford 
universal dependency annotation~\cite{DBLP:conf/lrec/MarneffeDSHGNM14} which currently is the 
largest collection of dependency treebanks in more than 40 languages. 

\subsection{Low-resource Dependency Parsing}
\label{sec:crosslingualDepParsing}
Consistent dependency treebank annotation typically requires careful guideline design, guideline testing and refinement, annotator quality control etc. %For Prague Dependency Treebank (PDT), it took a year for the first 1000 sentences~\cite{bohmovahhh:2001}, most of the time-consuming part is the annotation guideline and quality control. % but the next 20000 sentences took only a year too~\cite{bohmovahhh:2001}. 
We can not expect this high quality resource to be available for a resource-poor language. In this section, we review prior approaches to build a dependency parser for a resource-poor language.
 
\subsubsection{Delexicalization approach}
\label{sec:delexapproach}
This approach builds a delexicalized parser from a source language where a treebank is available. The delexicalized parser is built simply by removing lexical features from a supervised parser. This parser is then applied directly to the target resource-poor language. The underlying hypothesis is that aside from lexical items, other features are similar between two languages. 
Delexicalized parser was first proposed by~\namecite{Zeman08cross-languageparser}, who built a parser for Swedish using Danish, which are very similar languages.  They achieved a 66.4\% F1 labelled attachment score for Swedish. This is an encouraging result since they did not use any external resource such as a bilingual dictionary or parallel data. 
\namecite{McDonald:2011:MTD} also exploited the idea of a delexicalized parser. They experimented with 8 European languages. The main contribution of this paper stems from the incorporation of parallel data to the model. % Todo : can expand here, can include the predicted POS to the result 
\namecite{Sogaard:2011:DPS} is another example exploiting the delexicalized parser for a target language. Instead of choosing the source language that is similar to the target language, he chose data points from source languages that are similar to the target language with respect to POS sequences.

The above mentioned method for delexicalized parsers only used POS information.~\namecite{tackstrom:2013:NAACL-HLT} extended the POS features to other cross-lingual features. They adopted the WALS -- World Atlas of Language Structures~\cite{wals} --  typological features in the similar vein with~\cite{Naseem:2012:SSM}. WALS covers basic information such as order of \textit{Subject}, \textit{Object}, \textit{Verb}; order of \textit{Adjective} and \textit{Noun}; order of \textit{Adposition} and \textit{Noun} in  about 2,700 languages. Moreover, they also apply self-training and ensemble-training for relexicalized the delexicalized model. 

\subsubsection{Projection approach}
In contrast to the approach using language relatedness clues, we now investigate methods exploiting parallel data to either project the annotation from the source to the target language or as model constraint. 

\namecite{Hwa:2005:BPV} was the first to exploit this idea. The key assumption of this paper was the direct correspondence hypothesis between parallel text, which states that if $a$ was aligned to $b$ and $c$ was aligned to $d$, the dependency relation between $a$ and $c$ was likely to hold between $b$ and $d$ as well. Using this assumption, they defined a set of actions for each of the one-to-one, one-to-many, many-to-one, one-to-null or many-to-many word alignments. Given a source-language parse tree and the word alignment, they applied a series of predefined operations to generate the target-language parse tree. However, the performance of their direct transfer method was quite poor. They resolved this by applying a set of post-processing rules which captured the language specific knowledge. They achieved 72.1\% and 53.9\% UAS for Spanish and Chinese respectively. The approach of~\namecite{Hwa:2005:BPV} contained many heuristics and rules, which would be difficult to adapt to different languages. 

\namecite{Tackstrom:2012:CWC} built a delexicalized parser but additionally used cross-lingual word clustering induced from parallel data as a feature. The algorithm used to induce cross-lingual word clusters was an extension of the original Brown algorithm~\cite{Brown:1992}. 

\namecite{P14-1126} transferred the parameters of dependency parsers from source language to target language using parallel data and target language monolingual data. They trained a supervised English dependency parser as the source parser. They optimized the objective function that (1) minimizes the uncertainty of the target language using monolingual data and ensures (2) the distribution of the target parser is similar to the source parser according to the word alignments. 

\begin{table}
\begin{tabular}{llllllll|l}
\toprule
                & de   & el   & es   & it   & nl   & pt   & sv   & Avg (7) \\
\midrule        
\namecite{Zeman08cross-languageparser} & 47.2 & 63.9 & 53.3 & 57.7 & 60.8 & 69.2 & 58.3 & 58.6    \\
\namecite{Tackstrom:2012:CWC}    & 50.7 & 63.0   & 62.9 & 68.8 & 54.3 & 71.0   & 56.9 & 61.1    \\
\namecite{McDonald:2011:MTD}  & 50.9 & 66.8 & 55.8 & 60.8 & 67.8 & 71.3 & 61.3 & 62.1    \\
\namecite{P14-1126}   & 57.3 & 67.4 & 60.3 & 64.0   & 68.2 & 75.1 & 66.7 & 65.6    \\
\namecite{tackstrom:2013:NAACL-HLT}    & 61.5 & 69.6 & 66.9 & 73.4 & 60.2 & 79.9 & 65.5 & 68.1    \\
\bottomrule
\end{tabular}
\caption{Unlabelled attachment score (UAS) of different models across seven languages.}
\label{tab:summaryPerformance}
\end{table}
\subsection{Summary of Approaches}
There are two main approaches for low-resource dependency parsing using delexicalized parser and projection. Table~\ref{tab:summaryPerformance} summaries the performance of different models across seven common languages. The models listed in Table~\ref{tab:summaryPerformance} are sorted in the ascending average performance. %Direct transfer is the delexicalized model of~\namecite{McDonald:2011:MTD}. 
These approaches differ in their resource requirements. The baseline delexicalized parser~\cite{Zeman08cross-languageparser} only requires the same  POS tagset between source and target language.~\namecite{Tackstrom:2012:CWC} requires large parallel corpora for inducing cross-lingual word clusters.~\namecite{McDonald:2011:MTD} and ~\namecite{P14-1126} also need parallel data to constrain the model,  however, the amount of parallel data required is much less.~\namecite{tackstrom:2013:NAACL-HLT} did not use any parallel data however they combine multiple source languages for a single target language. All in all, the common denominator among these approaches is that they all start with the delexicalized model. In our \conllv\ paper, we propose a method to improve the delexicalized parser using no additional resources which is complementary to the other methods. In our \aclv\ and \emnlpv\ papers, we further improve the performance by combining a delexicalized parser with a model trained on a small annotated treebank. This gives a big boost in accuracy, especially in the low-resource scenario. 

\section{Crosslingual Word Embeddings}
Learning crosslingual word embeddings is the third task we consider in this thesis. Crosslingual word embeddings represent lexicons from several languages in the same dense vector space, which can be very useful for crosslingual NLP applications in a transfer learning setting. The delexicalized parser mentioned in \S\ref{sec:delexapproach}  is an example of transfer learning. Prior work on delexicalized parsing ignores all lexical features since they are different across languages. However, with the help of crosslingual word embeddings, we can map  lexicons from different languages to the same space. Then we can add lexical features back to the model, which is shown to improve  performance in our \conllv\ paper. 

\subsection{Monolingual Word Embeddings}
Since most crosslingual word embedding techniques are derived from monolingual word embedding methods, we first  review methods for building monolingual embeddings. 

% Distributed vs distributional 
Building monolingual word embeddings is the extension of the conventional count-based word vector space model based on the distributional hypothesis~\cite{Harris:54}. This hypothesis states that the meaning of a word can be induced from the surrounding context. Therefore, each word type can be represented as a vector of co-occurrence counts with words in the local context. Latent semantic analysis or singular value decomposition is usually applied to this vector to reduce the dimension which is usually the size of vocabulary~\cite{blacoe-lapata:2012:EMNLP-CoNLL,baroni-dinu-kruszewski:2014:P14-1}. Word embeddings learn a distributed representation of a word as opposed to the conventional distributional representation. While distributional representations collect word co-occurrence counts, word embeddings are usually formalized as a supervised machine learning task to predict the word that appears in a given context~\cite{Collobert:2008,mikolov-yih-zweig:2013:NAACL-HLT,Bengio:2003:NPL:944919.944966,Turian:2010:WRS:1858681.1858721,Huang:2012:IWR:2390524.2390645,pennington2014glove}.

% Success of word embeddings 
Despite much of the research into word embeddings being so new, word embeddings have 
widespread success in many applications such as natural language 
understanding~\cite{Collobert:2008}, sentiment analysis~\cite{socher-EtAl:2013:EMNLP},
dependency parsing~\cite{dyer-EtAl:2015:ACL-IJCNLP} and machine 
translation~\cite{DBLP:journals/corr/BahdanauCB14}. 
% List of notable works Senna, word2vec, Glove
The above work builds on the pioneering approach of~\namecite{Bengio:2003:NPL:944919.944966} 
who learn word embeddings as part of training a neural language model. The main drawback of 
this approach is that it is too slow to train on a big dataset since the classification objective function is 
normalized over the vocabulary size which is usually big.~\namecite{Collobert:2008} use down-stream 
tasks such as POS tagging, named entity recognition, noun-phrase chunking as well as  language modelling 
for learning shared compatible word embeddings across tasks. 
More recently,~\namecite{NIPS2013_5165} propose a vector log-bilinear language model (vLBL) and invert vector log bilinear language model (ivLBL) for learning word embeddings as the by-product of a neural language model.
~\namecite{mikolov-yih-zweig:2013:NAACL-HLT} proposed two embedding learning models, continuous bag-of-word (CBOW) 
and SkipGram model, extending~\namecite{NIPS2013_5165}. In both vLBL and CBOW 
models, the words in the context windows are used to predict the central word, while in the ivLBL and SkipGram 
 models, the central word is used to predict words in the context. 
Training in both~\namecite{NIPS2013_5165} and~\namecite{mikolov-yih-zweig:2013:NAACL-HLT} algorithms is fast 
thanks to hierarchical softmax and noise-contrastive estimation. Hierarchical softmax uses a tree-structure 
to compute the output probability reducing the complexity to logarithm of vocabulary 
size. Noise contrastive estimation and negative sampling applies for the unnormalized model, 
discriminating between samples from training data and samples from some noise distributions. 
This effectively reduces the algorithm complexity at each time step from $\mathcal{O}(n)$ where $n$ is the vocabulary size (e.g. 100k) to $\mathcal{O}(m)$ where $m$ is the number of samples which is usually small (e.g. 5). 
However, as the context window slides through 
the training data, training in both~\namecite{mikolov-yih-zweig:2013:NAACL-HLT} 
and~\namecite{NIPS2013_5165} is still proportional to the corpus 
size.~\namecite{pennington2014glove} proposed global vector model (GloVe) that works directly on the 
global pre-computed word co-occurrence statistics. In this way, they can scale the training time independently of the corpus size. 

% Extension of word embeddings 
% 	Trained on dependency tree 
% 	Sense embeddings 
%	more information to the embeddings (e.g. synset and lexemes)
% sentence embedddings
There are many other variations of word embeddings. Word embeddings are usually trained on monolingual data capturing word-context relations. 
However,~\namecite{chen-zhang-zhang:2014:Coling} trained the embeddings on the dependency treebank, which instead captures the head-modifier 
relation.~\namecite{rothe-schutze:2015:ACL-IJCNLP} adds information from knowledge base such as WordNet~\cite{Miller:1995:WLD:219717.219748} to the word 
embeddings.~\namecite{iacobacci-pilehvar-navigli:2015:ACL-IJCNLP}, ~\namecite{chen-liu-sun:2014:EMNLP2014} and~\namecite{tian-EtAl:2014:Coling} learn the sense 
embeddings instead of word embeddings since a word might have several senses. Other work covers phrase, sentence and document 
embeddings~\cite{DBLP:journals/corr/KirosZSZTUF15,DBLP:journals/corr/TaiSM15,kalchbrenner-grefenstette-blunsom:2014:P14-1,DBLP:conf/icml/LeM14}. Extending monolingual word embeddings to crosslingual word embeddings is another trend which will be discussed in more detail in~\S\ref{sec:xlingualemb}.
	
\subsubsection{Evaluation}
Monolingual word embeddings are usually evaluated on word similarity tasks. Given tuples of $(\text{word}_1,\text{word}_2,\texttt{s})$ where \texttt{s} is a scalar denoting the semantic similarity between $\text{word}_1$ and $\text{word}_2$ given by human annotators,  good word embeddings should produce a similarity measure e.g.\ cosine that is correlated with human judgement. There are many datasets testing different syntactic and semantic relations such as WordSim353~\cite{ws353}, RareWord~\cite{Luong-etal:naacl15:bivec}, MEN~\cite{DBLP:conf/acl/BruniBBT12} and SimLex-999~\cite{DBLP:journals/coling/HillRK15}. 

Monolingual word embeddings are also often evaluated on analogy tasks proposed by~\namecite{mikolov-yih-zweig:2013:NAACL-HLT}. This task aims at answering the question ``\texttt{a} is to \texttt{b} as \texttt{c} is to \texttt{d}'' where \texttt{a,b,c} are given and the system must predict \texttt{d}. For example, system must answer ``Japan'' to the following question ``Paris is to France as Tokyo is to what$?$''. There are two main datasets for this task, the MSR dataset~\cite{mikolov-yih-zweig:2013:NAACL-HLT} and the Google dataset~\cite{DBLP:journals/corr/abs-1301-3781}\footnote{They are constructed automatically from database tables.}. 

% Comparing them 
~\namecite{Levy_TACL570} and~\namecite{baroni-dinu-kruszewski:2014:P14-1} explain in detail and compare embeddings models with respect to the count-based methods in a controlled setting. They observed that CBOW and skipgram with negative sampling achieved consistently high results across different settings. This is why we extended CBOW with negative sampling in both our \emnlpvi\ and \eaclvii\ papers. 

%\subsubsection{CBOW with Negative Sampling}
%\tofix{SHOULD I PUT IT HERE ?? } OBJECTIVE FUNCTION, DERIVATION, NEGATIVE SAMPLING ....
% HERE %%%%


\subsection{Building Crosslingual Word Embeddings}
\label{sec:xlingualemb}
% Bilingual Word Embeddings
%It is common that unsupervised word representation help to improve the overall accuracy. I.e. include word cluster as a feature in supervised NLP system. 
There is a wealth of prior work on crosslingual word embeddings, which all exploit some kind of bilingual resource.
%LD: thesis mention this \tofix{LD:Probably should mention some distributional approach.}
This is often in the form of a parallel bilingual text, using word alignments as a bridge between tokens in the source and target languages. Usually, the translations are assigned similar embedding vectors~\cite{Luong-etal:naacl15:bivec,klementiev-titov-bhattarai:2012,zou-EtAl:2013:EMNLP}. 
\namecite{klementiev-titov-bhattarai:2012} and~\namecite{zou-EtAl:2013:EMNLP} build a alignment matrix $\textbf{A}$ of size $|V_e| \times |V_f|$ where $V_e$ and $V_f$ are vocabulary of source and target language. This matrix is then used to relate source and target embeddings as part of the training on parallel data.~\namecite{Luong-etal:naacl15:bivec}, on the other hand, use the alignments directly by extending the SkipGram model from~\namecite{mikolov-yih-zweig:2013:NAACL-HLT}. They predict the target language context using source language word which is specified by the alignment. 

These approaches are affected by errors from automatic word alignments, motivating other approaches which operate at the sentence level~\cite{Chandar-nips-14,DBLP:journals/corr/HermannB14,icml2015_gouws15}.~\namecite{DBLP:journals/corr/HermannB14} learn compositional vector representations of source and target sentence from 
individual word embeddings and constrains that both sentences must have similar representation.~\namecite{Chandar-nips-14} extend their approach to 
focus on learning the monolingual word relations of the learned crosslingual embedding. They minimize the reconstruction cost from source to target, target to source, source to source and target 
to target jointly.~\namecite{icml2015_gouws15} adopted the idea but with monolingual constraints from external monolingual data. The word embeddings learned this 
way capture translational equivalence, despite not using explicit word alignments. Nevertheless, these approaches require large parallel corpora, which are not 
available for many language pairs.

\namecite{vulic-moens:2015:ACL-IJCNLP} use bilingual comparable text, sourced from Wikipedia. 
Their approach creates a pseudo-document by forming bag-of-words from the lemmatized nouns in each comparable document concatenated over both languages.
These pseudo-documents are then used for learning vector representations using the skip-gram model from \texttt{Word2Vec}.
Their system, despite its simplicity, performed surprisingly well on a bilingual lexicon induction task. 
Their approach is compelling due to its reduced resource requirements, although comparable bilingual data can be scarce for many languages too. 
Related to this,~\namecite{sogaard-EtAl:2015:ACL-IJCNLP} exploit the comparable part of Wikipedia. They represent word using Wikipedia entries which are the same for many languages.\footnote{For example, Wikipedia entry Barack Obama is shared for all languages.} 

A bilingual dictionary is an alternative source of bilingual information.
\namecite{gouws-sogaard:2015:NAACL-HLT} randomly replace the text in a monolingual corpus with a random translation. They use this substituted corpus for learning word embeddings. 
Their approach doesn't handle polysemy, as very few of the translations for each word will be valid in context. 
They maximize the probability of a word given context $p(w_i|h)$ where $w_i$ is the middle word and $h$ is computed from $2k$ surrounding words $\{w_{i-k}, w_{i-k+1}, ... , w_{i+k-1}, w_{i+k}\}$. 
Assuming that each word in the context of window $k$ have $q$ translations, there can be as much as $q^{2k}$ possible contexts and  many are  incorrect. 
For this reason a high coverage or noisy dictionary with many translations might lead to poor outcomes.
\namecite{DBLP:journals/corr/MikolovLS13},~\namecite{W14-1613} and~\namecite{faruqui-dyer:2014:EACL} filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, however discarding much of the information in the dictionary. 
Our approach in \emnlpvi\ and \eaclvii\ also uses a dictionary, however we use all the translations and explicitly disambiguate translations during training. 

Aside from the bilingual data requirement, another distinguishing feature of the related work is the method for training embeddings.
\namecite{DBLP:journals/corr/MikolovLS13} and~\namecite{faruqui-dyer:2014:EACL} use a cascade style of training where the word embeddings in both source and target 
language are trained separately and then combined later using the dictionary.~\namecite{DBLP:journals/corr/MikolovLS13} learn the linear transformation to transform 
the source embeddings to the same space with the target embeddings.~\namecite{faruqui-dyer:2014:EACL}, on the other hand, use canonical correlation analysis to map 
both source and target embeddings to the same space. Most of the other work trains multilingual models jointly where the embeddings of both source and target language are 
learned together satisfying some constraints~\cite{Chandar-nips-14,DBLP:journals/corr/HermannB14,icml2015_gouws15,Luong-etal:naacl15:bivec}. This appears to have better performance over cascade training~\cite{icml2015_gouws15}. For this reason we also use a form of joint training in this thesis. 

\begin{table}[t]
\centering
\begin{tabular}{llcc}
\toprule
Paper                                        & Bilingual resource & External mono & Multi langs \\
\midrule
\namecite{zou-EtAl:2013:EMNLP}             & parallel corpus    & no            & no          \\
\namecite{klementiev-titov-bhattarai:2012} & parallel corpus    & no            & no          \\
\namecite{Luong-etal:naacl15:bivec}        & parallel corpus    & yes           & no          \\
\namecite{Chandar-nips-14}                 & parallel corpus    & no            & no          \\
\namecite{DBLP:journals/corr/HermannB14}   & parallel corpus    & no            & no          \\
\namecite{icml2015_gouws15}               & parallel corpus    & yes           & no          \\
\namecite{vulic-moens:2015:ACL-IJCNLP}     & comparable corpus  & no            & no          \\
\namecite{gouws-sogaard:2015:NAACL-HLT}    & dictionary         & yes           & no          \\
\namecite{DBLP:journals/corr/MikolovLS13}  & dictionary         & yes           & no          \\
\namecite{faruqui-dyer:2014:EACL}          & dictionary         & yes           & no          \\
\namecite{W14-1613}                        & dictionary         & yes           & no          \\
\midrule
\namecite{sogaard-EtAl:2015:ACL-IJCNLP}    & Wikipedia entries  & no            & yes         \\
\namecite{coulmance-EtAl:2015:EMNLP}       & parallel corpus    & yes           & yes         \\
\namecite{DBLP:AmmarMTLDS16}               & dictionary         & yes           & yes         \\
\namecite{huang-EtAl:2015:EMNLP}           & parallel corpus    & no            & yes         \\
\bottomrule
\end{tabular}
\caption[Summary of crosslingual word embeddings papers]{Summary of crosslingual word embeddings papers according to the bilingual resources used, support for incorporation of external monolingual data and support for extension to multiple languages.}
\label{tab:clwe_papers_summary}
\end{table}

Another important factor for crosslingual word embeddings is the ability to extend to multiple languages. 
Previous work mainly focuses on building word embeddings for a pair of languages, typically 
with English on one side, with the exception of \namecite{coulmance-EtAl:2015:EMNLP}, \namecite{sogaard-EtAl:2015:ACL-IJCNLP} and~\namecite{DBLP:AmmarMTLDS16}. 
\namecite{coulmance-EtAl:2015:EMNLP} who extend the bilingual skipgram model from~\namecite{Luong-etal:naacl15:bivec}, training jointly over many languages using the Europarl corpora. Instead of using the source language word to predict the target language context, they jointly predict target language context in multiple languages. 
~\namecite{huang-EtAl:2015:EMNLP} who also using bilingual corpora, exploit the fact that crosslingual word embeddings must be invariant to translation between languages. However, parallel data is an expensive resource  for many low-resource languages. 
While~\namecite{coulmance-EtAl:2015:EMNLP} use English as the pivot language, \namecite{sogaard-EtAl:2015:ACL-IJCNLP}
learn multilingual word embeddings for many languages using Wikipedia links.\footnote{For example, the page about Barack Obama in different languages are all linked together.}
However, their approach is limited to languages covered in Wikipedia.
\namecite{DBLP:AmmarMTLDS16} propose two algorithms namely MultiCluster and MultiCCA 
for multilingual word embeddings using set of bilingual dictionaries. MultiCluster first 
builds the graph where nodes are lexicon and edges are translations. Each cluster in this 
graph is an anchor point for building multilingual word embeddings. MultiCCA is an extension 
of~\namecite{faruqui-dyer:2014:EACL}, performing canonical correlation analysis (CCA) for 
multiple languages using English as the pivot. A shortcoming of MultiCCA is that 
it ignores polysemous translations by retaining only one-to-one dictionary 
pairs, disregarding much information~\cite{icml2015_gouws15}. 

Table~\ref{tab:clwe_papers_summary}
summaries crosslingual word embedding papers and their differences in term of resource usage,
ability to incorporate monolingual data and extension to multiple languages. Incorporation of monolingual data is 
important for capturing monolingual similarity, however, some methods are not capable of doing so 
mostly because of a complicated objective function~\cite{Luong-etal:naacl15:bivec}. As mentioned above, extending to multiple languages is 
also desirable as we have a shared space for multiple languages enabling multilingual 
applications such as multi-source machine translation~\cite{zoph-knight:2016:N16-1} and 
multi-source transfer dependency parsing~\cite{McDonald:2011:MTD}. Our work starts by building crosslingual word embeddings 
for a pair of language in our \emnlpvi\ paper using a broad coverage yet noisy dictionary from Panlex and  monolingual data.
In this way, our approach can be applied to more languages as PanLex covers more than a thousand languages. 
We extend to multiple languages, jointly learning multilingual word embeddings in our \eaclvii\ paper.

\subsection{Evaluation}
Crosslingual word embeddings can be tested using intrinsic and extrinsic evaluations. Intrinsic evaluations test the word representations, aiming at checking if the relative distances between words in the embedding space satisfy some linguistic constraints. 
The monolingual distances are usually tested in the same way as monolingual word embeddings, using
monolingual word similarity and monolingual word analogy datasets. 

The bilingual distance can be tested in several ways.~\namecite{camachocollados:2015:ACL-IJCNLP} propose 
several crosslingual word similarity datasets, similar to monolingual word similarity dataset, containing tuples 
of $(\text{word}_1,\text{word}_2,\texttt{s})$, however $\text{word}_1$ and $\text{word}_2$ are in different languages and $\texttt{s}$ is the similarity given by human judgement.~\namecite{vulic-moens:2015:ACL-IJCNLP} propose to test the bilingual distance using the bilingual lexicon induction task. 
Given a word in a source language, the bilingual lexicon induction task is to predict its translation in the target language 
using the crosslingual word embeddings. The difficulty of this task is that it is evaluates using the recall of the top ranked word. 
The model must be very discriminative in order to score well.

The extrinsic evaluations test the usefulness of crosslingual word embeddings on downstream tasks.~\namecite{klementiev-titov-bhattarai:2012} propose a
crosslingual document classification task. In this task, the document classifier is trained on a source language and then applied directly 
to classify a document in the target language. This is convenient for a target low-resource language where we do not have document annotations. 
The documents are represented as bag of word embeddings weighted by \texttt{tf.idf}. Thanks to  the crosslingual word embeddings, the document 
representation in the target language is in the same space with the source language, enabling transfer learning. Crosslingual dependency 
parsing is another commonly used task evaluating crosslingual word embeddings~\cite{DBLP:AmmarMTLDS16,upadhyay-EtAl:2016:P16-1}. In this setup, 
the source language parser is trained on the source language treebank using only word embeddings i.e. removing all the other features 
such as part-of-speech and morphology. The source language parser is applied directly to the target language. By removing all other features, 
this evaluation emphasize the contribution of crosslingual word embeddings. 

\section{Unwritten Language Processing}
% Connecting with previous section 
As mentioned earlier, our chosen tasks should be representative for processing low-resource languages. 
We want to cover both semantic and syntactic tasks with multiple input formats. In this section, we discuss the fourth task, concerning the 
extreme case of unwritten language processing. This task is substantially different to previous tasks as we did not assume any writing system  for the target language. 
However, this is the common scenario since at least half of the 7000 languages in the world do not have an established orthography~\cite{lewis2009}. This leaves us with 
no other choice than to work directly with the speech signal. Unlike the previously tasks, processing unwritten language is a relatively new task. That is why 
in this section, we will review the current techniques and data requirements for unwritten language processing first before proposing our task. 
% What is the task ? 

\subsection{Unsupervised segmentation and lexical discovery}
\label{sec:unsuperived_segmentation}
The input to this task is raw speech in a low-resource language and the system must 
be able to segment the continuous speech signal to find word boundaries and detect repeated lexical items.
Infants are very competitive at this task even during their first year. 
Toward the end of their first year, they can distinguish between phonetic contrasts (i.e. consonants and vowels), 
start segmenting continuous speech into words and understand a few words even before starting to talk~\cite{RaSaNen:2012:CMP:2318326.2318449}. 
While computer systems struggle with this task, infants do this naturally without any direct supervision even in the presence of environmental noise. 
The zero-resource speech processing challenge~\cite{Versteegh201667} is the task defined to ``reverse-engineer'' this ability from infants. 
In this zero-resource setting, the model must jointly learn a representation of the speech signal which helps to distinguish between different 
linguistic units such as word or phone and then group speech into meaningful words. This will be useful for many tasks such as 
voice query over raw speech signal~\cite{Park:2007:UPD:1329638} or unsupervised term detection~\cite{6163965}.

\paragraph{Unsupervised term detection (UTD)} is the task of finding meaningful spoken words or phrases in a speech signal. Most  approaches extend 
segmental dynamic time warping proposed by~\namecite{Park:2007:UPD:1329638}. Dynamic time warping (DTW)~\cite{Berndt:1994:UDT:3000850.3000887} is the technique based on dynamic programming to calculate the 
distance between two temporal sequences of speech of variable length. Segmental DTW calculates the distance based on segments of speech rather than the whole sequence. 
Most work on UTD builds the graph where each node is a speech segment and the 
edge is weighted by applying segmental DTW between nodes. Moreover,~\namecite{6289082} improve on robust speech feature representation using a Boltmann machine.~\namecite{Lyzinski2015AnEO} experiment with different graph clustering algorithms and~\namecite{6163965} focus on improving efficiency. 

UTD aims at segmenting and finding repetitive spoken terms for a small subset of vocabulary where the words or phrases are frequent. Full-coverage term discovery, on 
the other hand, aims at segmenting and clustering the whole vocabulary~\cite{Lee:TACL15,KamperJG16a,Kamper2015FullyUS,RasanenDF15}.~\namecite{Lee:TACL15} build 
the pseudo-phone acoustic model with the speech segmentation learned as part of training. They also include a noisy-channel to model  phonological variability (i.e. 
difference between phonetic context and stress) and exploit adaptor grammars~\cite{JohnsonGG06} to group several pseudo-phone units into syllables and words. 
When trained on the English MIT lecture corpus, most high \texttt{tf.idf} words are discovered.~\namecite{Kamper2015FullyUS} observed that if we have the speech segmentation, we 
can convert each segment into a fixed size vector and then the term discovery task will be reduced to clustering, which can be done using a Gaussian Mixture Model (GMM) 
acoustic model. Moreover, if we have the GMM acoustic model, we can use dynamic programming for finding the speech segmentation. This is the motivation for their 
Bayesian sampling model for jointly learning the segmentation and GMM acoustic model. However, due to the complexity of the algorithm, they only experiment on a small 
vocabulary dataset of digits from the TIDigits dataset~\cite{Tidigit_dataset}.~\namecite{KamperJG16a} extend~\namecite{Kamper2015FullyUS} to be able to run on bigger 
vocabulary. Instead of sampling for all possible segmentations,~\namecite{KamperJG16a} only sample from a small pre-defined set of segmentations as the outputs 
from~\namecite{RasanenDF15}. Also, they employ a much simpler method for computing acoustic embeddings from speech features based on down-sampling which is basically a 
technique for averaging with some smoothing. These modifications allow them to scale up to large vocabulary unsupervised term discovery and score well on zero-resource challenges. 

\paragraph{Speech segmentation} is usually a part of unsupervised term detection as segmentation is jointly induced. However, speech segmentation can be done separately to provide the system 
with candidate word boundaries.~\namecite{Khanagha:2014:PSS:2844738.2844801} use microcanonical multiscale formalism to segment speech analysis to phone-like 
units. Currently, they achieved state-of-the-art performance on the phone segmentation task for the English TIMIT dataset~\cite{timit}.~\namecite{RasanenDF15} experimented 
with several speech segmentation methods including (a) VSeg which determines syllable boundaries based on velocity of low-pass filtered amplitude envelope~\cite{nuimeprn1268},
(b) envelope minima detector which find rhythm-based segmentation~\cite{villing2006performance} and (c) amplitude envelope-driven 
oscillator~\cite{ghitza2011linking}. Both~\namecite{RasanenDF15} and~\namecite{Khanagha:2014:PSS:2844738.2844801} focus on recall rather than precision, that is they all over-segment the speech signal. However, these segmentations provide the list of candidates for further applications~\cite{KamperJG16a}. In our~\naaclvi\ paper, we also experimented with speech pre-segmented using the algorithm of~\namecite{Khanagha:2014:PSS:2844738.2844801}.

\paragraph{Speech feature extraction} is usually the first step when working with speech. Feature extraction converts the digital waveform into feature frames. 
This reduces the variability of speech such as pitch, excitation and speaker differences, which makes it easier to process. Mel-frequency cepstral coefficients (MFCC) can be 
considered as a standard feature extraction method, deriving the cepstral representation of audio~\cite{Fang:2001:CDI:569577.569587}. People usually use 12 
coefficients with energy together with first and second derivative to capture the change over time resulting in a 39 dimensional vector. This is usually calculated for 
25 millisecond frame length with 10 millisecond frame step. However, MFCC is criticized for its sensitivity for noise~\cite{DBLP:journals/corr/abs-1305-1145}. 
In our~\naaclvi\ paper, we represent the speech waveform as a sequence of Perceptual Linear Prediction (PLP) vectors~\cite{Hermansky90plpspeech}. PLP features encode the 
power spectrum of the speech signal with the focus on minimizing speaker differences and noise. 

Recently, deep neural network inspired features are becoming 
popular over traditional representations such as MFCC or PLP.~\namecite{HermanskyTandem} propose tandem features derived from the top layer of a multilayer perceptron 
trained to classify phones. Tandem features represent the probability of each phone given the input. That is why tandem features have the size equal with number of  
phones.~\namecite{GrezlBottleNeck} proposed bottle-neck features which use the middle layer of a bigger multilayer perceptron as the representation instead of the 
output layer. The bottle-neck layer has a much smaller size compared with the input and other hidden layers, condensing the information.~\namecite{Kamper15AE} propose auto-encoder-based features with top-down constraints. They use a separate UTD to get a set of word pairs that served as weak top-down supervision. 
The word pairs are aligned to get frame level alignment using dynamic time warping. Each frame-level pair (e.g. \texttt{A} and \texttt{B}) are used to train an 
auto-encoder to minimize the reconstruction cost from \texttt{A} to \texttt{B} and vice versa. A middle layer of this auto encoder is used as the feature representation. In this way, the feature representation is likely to be better at discriminating words. 

The input layer for the deep neural network feature representation is the conventional representation such as MFCC or PLP, but the output is usually more robust to noise, speaker difference and environmental conditions. Moreover, neural network based feature extraction can be used to bootstrap for a low-resource language. For example, features 
extracted with~\namecite{HermanskyTandem} or~\namecite{GrezlBottleNeck} need phone annotation since they are trained to classify phones. This annotation might not be available or very limited for a target low-resource language. However, based on phone annotation from several source languages, we can apply deep neural network feature extraction to build the phone-discriminative representation in the target low-resource language~\cite{Vesely12,Stolcke06Share,ThomasMLPfeatures12}. 

\subsection{Speech recognition for low-resource language}
The input for unsupervised lexical discovery is just the speech signal. This is suitable for low-resource language and even unwritten language. 
However, unsupervised term detection is not enough to analyse a language. In this section, we will review low-resource language speech recognition which is not particularly suitable for 
unwritten languages unless the writing system is invented.\footnote{As done for Levantine Arabic and Iraq Arabic as part of DARPA projects} However, the task we propose later (\S\ref{sec:proposed_tasks_unwritten}) is similar to low-resource speech recognition which is why we review it here. 

\subsubsection{Conventional approach}
Speech recognition outputs a text transcription from a speech waveform. Conventional approaches for speech recognition exploit Hidden Markov Model (HMM) architectures. 
There are three main components of conventional HMM based speech recognition which are an acoustic model, a lexical model and a language model. At a high level, the acoustic model converts speech signal to list of sub-word (e.g. phoneme or syllable) representations. The lexical model builds the pronunciation dictionary to convert lists of sub-word units into words. The language model ranks word sequences to find the most probable output sentence. 

\paragraph{Acoustic models} aim at computing the likelihood of some representation given the speech waveform. The representation can be mono-phone or syllable for a context-independent 
acoustic model or tri-phone or penta-phone for a context-dependent acoustic model. The purpose of this step is to reduce variability and complexity of the speech signal. 
It is easier to work with some written form rather than directly with the waveform. The first step of the acoustic model is feature extraction as mentioned earlier (\S\ref{sec:unsuperived_segmentation})
using methods such as MFCC, PLP, bottle-neck or tandem features. In the conventional context-independent HMM-phoneme based acoustic model, the sequence of feature 
frames are converted to a phoneme representation. The system is trained on the phoneme transcription which is usually the 
output of manual labelling (extremely time-consuming) or mapping between pronunciation dictionary and word transcription (more practical). However, the phone annotated 
transcriptions might not be available for many low-resource languages. The acoustic model can be ported from higher resource languages.~\namecite{Le2009Vn} 
proposed several methods for phone mapping between source and target languages, successfully applied for low-resourced language 
Vietnamese.~\namecite{Siniscalchi2013209} proposed a set of shared fundamental units that is universal across languages, facilitating  acoustic model 
sharing. The acoustic model is trained on the source language and applied directly to the target language.~\namecite{Stuker2003} proposed similar set but based on International Phonetic Alphabet (IPA).  %These multilingual acoustic model can be very helpful for a low-resource language providing the phonene lattice There are also many speech related work that 
%by-pass the complexity of speech signal processing by assuming the availability of phoneme sequence or phoneme lattice as the output of 
%multilingual acoustic model~\cite{stahlberg2012word,adams-EtAl:2016:EMNLP2016}.

\paragraph{Lexical model} is used to create pronunciation dictionary, specifying the decomposition of word to sub-word spoken unit (e.g. phoneme). This pronunciation 
dictionary is a crucial resource for the acoustic model. Usually, pronunciation dictionary is built manually. The crowd-sourced dictionary from 
Wiktionary,\footnote{Wiktionary.org} providing phonemic annotation written in IPA, is a great source for a pronunciation dictionary~\cite{Schlippe2014101}. Instead of 
using phoneme units, a grapheme based approach can be used to build a pronunciation dictionary. This is very useful for languages where there is a close relation between 
grapheme and phonemes. For example, in Vietnamese, children can always pronounce an unknown word correctly given the written form. This approach has been applied 
successfully to extract a grapheme pronunciation dictionary for Vietnamese~\cite{Le2009Vn} and Thai~\cite{Stker2008IntegratingTG}. However, the problem with grapheme-based dictionaries is that the acoustic model has to work on grapheme too which are usually not shareable across languages. Grapheme-to-phoneme mapping is a solution since it maps language specific 
grapheme back to universal phonemes. The mapping can be done manually or automatically using machine translation approaches~\cite{Karanasou:2010:CSM,Cucu2012}. 

\paragraph{Language models} prefer certain word order and word co-occurrences, helping to distinguish words or phrases that sound similar but may confuse lexical and acoustic models. The language model is usually trained on a large monolingual dataset using an n-gram language model or more recently proposed neural language model~\cite{Collobert:2008,mikolov-yih-zweig:2013:NAACL-HLT,Turian:2010:WRS:1858681.1858721,Huang:2012:IWR:2390524.2390645,pennington2014glove}. 

\subsubsection{Modern approach}
Conventional speech recognition requires the speech waveform, word transcription and a pronunciation dictionary for training. Modern approaches usually only require speech signal and the transcription which is made possible by exploiting the representation learning of a deep neural network.~\namecite{maas-EtAl:2015:NAACL-HLT} and ~\namecite{DBLP:conf/icassp/GravesMH13} use bidirectional recurrent neural networks with a connectionist temporal classification loss function~\cite{Graves06connectionisttemporal} to generate the transcription character by character.~\namecite{DBLP:journals/corr/ChorowskiBCB14} is the first to train an end-to-end speech recognition system based on the attentional model proposed originally for machine translation~\cite{DBLP:journals/corr/BahdanauCB14}. Unlike machine translation, there is no reordering in speech recognition. They add a constraint to prefer a monotonic alignment between speech frame and transcription. In our~\naaclvi\ paper, we also experimented with automatic speech recognition task and observed substantial improvement adding this monotonic constraint.~\namecite{DBLP:journals/corr/ChanJLV15} also use the similar sequence to sequence framework with attention mechanism but make it more robust by randomly introducing noise during training which leads to substantial improvement. They also introduce pyramidal structure for condensing speech features which we adopted for our~\naaclvi\ paper. All these modifications make their result close to the state-of-the-art HMM-based speech recognizer. Moreover, in term of resources, modern approaches are more suitable for low-resource languages since no pronunciation dictionary is required. 

\subsection{Low-resource Speech Data Collection}
The unsupervised term discovery task just requires the raw speech signal which can be cheaply collected for many low-resource languages through radio broadcasts, fieldwork 
or public speech. However, the raw speech waveform does not carry much information and the result of unsupervised term discovery is not very meaningful, that is why field workers usually collect some annotation for a low-resource 
language. Transcription is usually used for languages having some writing system.~\namecite{deVries2014119} introduces a smart phone-based data collection tool, 
Woefzela, to collect speech and transcription with the focus on quality control. They collected almost 800 hours of speech on their South African data collection 
project demonstrating the usefulness of smart phone devices to cheaply and efficiently collect data. Aikuma~\cite{bird-EtAl:2014:W14-22} is another smart phone application in this 
line of research. However, instead of transcription, they use Aikuma to collect the parallel speech between the unwritten language and a higher resource language for language preservation purpose. This 
is motivated by the fact that aside from their mother language, people usually speak another higher resource language. The higher resource language can be used to 
approximate the meaning of the unwritten language. By recording parallel speech, even if the unwritten language dies out, we still have a footprint of that 
language. For the initial experiment,~\namecite{bird-EtAl:2014:Coling} managed to collect around 10 hours of parallel speech from indigenous 
communities in Brazil and Nepal.~\namecite{Blachon201661} used an extended version of Aikuma to collect more than 80 hours of parallel speech from Congo-Brazzaville. 

\subsection{The Proposed Task of Unwritten Language Processing}
\label{sec:proposed_tasks_unwritten}
Unsupervised term discovery has limited utility and automatic speech recognition is not suitable for many unwritten languages. Given the availability of tools and data initially collected by~\namecite{bird-EtAl:2014:Coling} and~\namecite{Blachon201661}, it is desirable to be able to model the relationship in the parallel speech between an unwritten language and the target higher resource language. Moreover, since the target language is high resource, we can crowd-source or apply automatic speech recognition for cheaply obtain a target language transcription. If we do this, we reduce the task to speech translation where the source is the unwritten language speech and the target is the translation text in the target language. This is the setting of our proposed task. We want to model and learn useful relations between speech in the low-resourced, unwritten language and the translations in the target higher resource language. 

% Closely related to speech recognition
This is closely related to the speech recognition task where instead of the transcription in the same language, we use a different language translation to understand the semantics of the speech. However, this poses a challenge partly due to the monotonic constraint no longer holding, i.e.~the translation word order is more fluid and does not follow the source word order. In our~\naaclvi\ paper, we exploit a deep neural network approach using an attentional architecture. We show that we can learn meaningful relations directly from this data.   

\vspace*{2ex}
In this chapter, we have provided background and literature reviews for tasks of interest. We started by defining what is a low-resource language focusing on tasks. We enumerated possible resources for a low-resource language. This helps to estimate what we can reasonably expect when working under low-resource scenario. We also give the general picture of low-resource natural language processing including common techniques such as transfer learning and problems such as speech segmentation and named entity recognition, before diving into four proposed tasks. For each task, we review prior work to understand the gap between low-resource and high-resource settings and to better position our work proposed in the next chapter (\S\ref{chap:research_summary}) relative to the literature. Where possible, we made a systematic  comparison focusing on performance and resource requirements. 
%%%%%%%%%%%%%%%%%%%%%%%%NEW CHAPTER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Research Contribution}
\label{chap:research_summary}
% Remind people what the thesis is about (2 pages) 
This chapter is the main contribution of our thesis targeting at building natural language processing (NLP) framework for low-resource languages. 
% List 4 tasks 
Due to the limited scope of the thesis, we focus on four NLP tasks 
for low-resource languages including (1) part-of-speech  tagging, (2) dependency parsing, (3) cross-lingual word embeddings and (4) unwritten 
language processing. These tasks are selected to be representative for a language covering both semantic and syntactic perspectives, also 
multi-modal inputs including both text and speech. We believe that these tasks are crucial to process a low-resource language.  
% Approaches, methodology with transfer learning 
Due to lack of annotated data for building high quality supervised model, we take the approach of 
unsupervised or semi-supervised learning in the transfer learning framework for many proposed tasks in this thesis. Specifically, we have successfully transferred the knowledge from a source resource-rich language to the target resource-poor languages resulting in several publications including~\emnlpiv, \aclv, 
\emnlpv, \conllv, \emnlpvi\ and \eaclvii . Moreover, processing unwritten language is a very challenging task where we have to work on speech signal directly. We learn the alignment and translation model directly between speech signal and the translation in the target high-resource language. This  
result a ~\naaclvi\ paper.
% Remind about research question ? 
% ??? 
% The structure of the chapter 
The set of publications form the backbone of this thesis and are the main contributions. 
In the following section, we will list those publications, for each publication we will give (1) the full bibliography, (2) the background research process and (3) the retrospective view with critical analysis of contribution toward the thesis target. 

%The rest of this chapter is organized as follow. First, we list all the publications in \S\ref{sec:publications} which form the back-bone of this thesis. 
%We then critically evaluate the contribution of this thesis with respect to the research question (\S\ref{sec:evaluation_contribution}), followed by 
%future work (\S\ref{sec:futurework}) and conclusion (\S\ref{sec:conclusion}). 
%\section{Publications}
%\label{sec:publications}
%In this section, 
% What will do for each publication 
% Target ? How related to thesis ? 
% Quick contribution 
% Retrospective view 
% Contribution 

%------------------------------------------------------------%
%------------------------------------------------------------%
%------------------------------------------------------------%


\section{EMNLP 2014 -- Low-resource POS Tagging}
\label{sec:emnlp14}
\begin{quote}
Long Duong, Trevor Cohn, Karin Verspoor, Steven Bird and Paul Cook. 2014. What Can We Get From 1000 Tokens? A Case Study of Multilingual POS Tagging for Resource-Poor Languages. In \textit{Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)}. 886--897, Doha, Qatar.
\end{quote}
\subsubsection{Research process}
Our prior work on low-resource natural language processing focuses on part-of-speech tagging using parallel data to project the annotation from a source language 
to a target language~\cite{Duongacl13,duongIJCNLP}. This work was conducted before my PhD, and for this reason, is not included in this thesis. In this work, we conclude that the performance of prior work
was low and had a high dependency on the quality and quantity of parallel data. Improving the performance and reducing data requirements  are the motivation for this paper. This paper, which is an extension of our prior work, fits nicely with the general thesis target, solving the first task, providing analysis about syntax. 

%In this paper, I implemented the algorithm and wrote the first draft of the paper. Other co-authors (my supervisors) contributed ideas during our weekly meetings and participated in the paper writing. 

\subsubsection{Retrospective view}
In this paper, we assume the availability of some small POS annotated corpus. We show that we can learn better model compared with purely 
supervised learning taking into consideration parallel corpora. It was surprising seeing that the model still works well for a low-resource language Malagasy where parallel corpora's size are modest . However, the main concern is that the work will be less relevant as there are more annotated 
data which is also shown in the paper.% Moreover, the cost for POS annotation is relatively cheap~\cite{garrette:naacl13}. Probably just simply annotate more data and 
%apply any simple supervised learning method will have better benefit-cost ratio. 
% Weak point of the paper 
% What have I learned ? 
%However, through this paper, I have learned more about maximum divergence model, conditional random field and probabilistic graphical model. 
% Annotated data is extremely important 

%\includepdf[pages={1-},scale=1,pagecommand={\thispagestyle{plain}}]{Papers/emnlp14.pdf}
\includepdf[pages={1-},offset=0mm -7mm,scale=0.95,pagecommand={}]{Papers/emnlp14.pdf}

%------------------------------------------------------------%
%------------------------------------------------------------%
%------------------------------------------------------------%

\section{ACL 2015 -- Low Resource Dependency Parsing}
\label{sec:acl15}

\begin{quote}
Long Duong, Trevor Cohn, Steven Bird, Paul Cook. 2015. Low Resource Dependency Parsing: Cross-lingual Parameters Sharing in a Neural Network Parser. In\textit{ Proceeding of the 53rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}.  845--850, Beijing, China
\end{quote}
\subsubsection{Research process}
% Joint training ...
After part-of-speech (POS) tagging, dependency parsing is a natural extension towards learning a deeper model of syntax. Dependency parsing is significantly 
harder than POS tagging. Instead of just outputting the tag label, we have to predict the tree structure of the sentence. Since we have successfully 
applied transfer learning for POS tagging taking advantage of small annotated corpus, a natural question is, can we do the same thing for dependency 
parsing? This paper forms a part of solving the second task of dependency parsing. 

In this paper, I designed and ran experiments. Other co-authors (my supervisors) contributed ideas during our weekly 
meetings and participated in the paper writing. 

\subsubsection{Retrospective view}
We observe that transfer learning technique through regularization terms is still applicable for dependency parsing task. Moreover, we only 
require the same POS annotation and dependency type between source and target languages. This is a much better 
assumption compared with parallel data as used in ~\emnlpiv\ paper. Nevertheless, this paper still suffers similar drawback with the assumption of 
a small annotated corpus in the target language. However, since annotating dependency treebank is much more costly and time consuming than POS annotation, it is more compelling to apply our technique instead of annotating more data.  

% Independent of parallel data 
% Cost for dependency is higher ... 
% We haven't tested the joint training (cascade training) 

\includepdf[pages={1-},offset=0mm -7mm,scale=0.95,pagecommand={}]{Papers/acl15.pdf}

%------------------------------------------------------------%
%------------------------------------------------------------%
%------------------------------------------------------------%

\section{EMNLP 2015 -- Universal Dependency Parsing}
\label{sec:emnlp15}
\begin{quote}
Long Duong, Trevor Cohn, Steven Bird, Paul Cook. 2015. A Neural Network Model for Low-Resource Universal Dependency Parsing. In \textit{Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}. 339--348, Lisbon, Portugal.
\end{quote}

\subsubsection{Research process}
In our \aclv\ paper, we trained the model in a cascade style where source language parser is trained first and then used as the prior for the target language parser. However, we might benefit more from jointly train the source and target language together as this allows better 
parameter sharing, which is the motivation for this paper. 

In this paper, I designed and ran experiments. Other co-authors (my supervisors) contributed ideas during our weekly 
meetings and participated in the paper writing. 

\subsubsection{Retrospective view}
As shown in the paper, joint training is substantially better than cascade training across various data size. Also, joint training is more flexible, we can 
even relax the requirement of the same POS or dependency type annotation imposed in cascade training in \aclv\ paper. The model can automatically learn the 
annotation mapping between source and target language as part of the training. However, it is usually slower and more challenging to efficiently train the 
join model. Moreover, in this paper, we mainly compare with prior work using similar neural network transitional based parsing approach. Despite the fact that the 
proposed joint model is generic and can apply to various architectures, it is an open question on how this work apply to or compare with prior work using different parsing 
architectures such as graph base. 
\includepdf[pages={1-},offset=0mm -7mm,scale=0.95,pagecommand={}]{Papers/emnlp15.pdf}

%------------------------------------------------------------%
%------------------------------------------------------------%
%------------------------------------------------------------%


\section{CoNLL 2015 --  Unsupervised Dependency Parsing}
\label{sec:conll15}
\begin{quote}
Long Duong, Trevor Cohn, Steven Bird, Paul Cook. 2015. Cross-lingual Transfer for Unsupervised Dependency Parsing Without Parallel Data. 
In \textit{Proceedings of the Nineteenth Conference on Computational Natural Language Learning (CoNLL 2015)}. 113--122, Beijing, China. 
\end{quote}

\subsubsection{Research process}
In the \aclv\ and \emnlpv\ papers, we assume a small annotated treebank in the target language. However, this treebank might not be available for 
many low-resource languages. That is why we want to further relax this assumption, motivating this paper for unsupervised dependency parsing. 

In this paper, I designed and ran experiments. Other co-authors (my supervisors) contributed ideas during our weekly 
meetings and participated in the paper writing. 

\subsubsection{Retrospective view}
% Make much different in the downstream application ? 
We noticed in our~\emnlpv\ paper that a delexicalized parser performs surprisingly well, achieving similar performance with supervised learning when trained on 3000 tokens. In this paper, we improve the delexicalized parser without using any additional data. This 
is done using syntactic word embeddings that only requires part-of-speech in the source and target languages which is the same data condition as before.  
language. However, for a single source language the improvement is modest. The biggest 
gain is from taking advantage of multiple source languages. However, the gain is not consistent across languages and usually higher for languages that share 
some commonalities with the source language. 

\includepdf[pages={1-},offset=0mm -7mm,scale=0.95,pagecommand={}]{Papers/conll15.pdf}


%------------------------------------------------------------%
%------------------------------------------------------------%
%------------------------------------------------------------%


\section{EMNLP 2016 -- Crosslingual Word Embeddings}
\label{sec:emnlp16}
\begin{quote}
Long Duong, Hiroshi Kanayama, Tengfei Ma, Steven Bird, Trevor Cohn. Learning Crosslingual Word Embeddings without Bilingual Corpora. In \textit{Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016)}. 1285--1295, Austin, Texas, USA\
\end{quote}

\subsubsection{Research process}
Transfer learning is core of all our previous papers, most notably via lexical transfer. Investigating better methods for lexical transfer would benefit not only POS tagging or dependency parsing but also many other transfer learning tasks, motivating this paper. 

This paper was done during my internship at IBM Research -- Tokyo where they interested in transfer learning for their cross-lingual data mining system. I undertook the 
internship mainly because their research interest aligns well with the overall target of the thesis, aiming at solving the third task (crosslingual 
word embeddings). Aside from A/Prof. Steven Bird and Dr. Trevor Cohn from Melbourne university, I also have Dr. Hiroshi Kanayama and Dr. Tengfei Ma as my IBM supervisors. In this paper, I implemented the algorithm and ran all experiments. Other co-authors contributed ideas during our weekly meetings and participated in the paper writing. 

\subsubsection{Retrospective view}
This paper uses dictionary for building crosslingual word embeddings applied successfully for both intrinsic tasks (monolingual similarity and bilingual lexicon
induction task) and extrinsic task (crosslingual document classification). However, because of space constrain, we haven't evaluated on syntactic extrinsic task such as 
crosslingual dependency parsing. Moreover, crosslingual word embeddings currently only work for a pair of language, it is shown in our~\conllv\ paper that crosslingual word embeddings for multiple languages are more beneficial in transfer learning. 
%In addition, the experiment with low-resource language Serbian need to compare with several other dictionary based cross-lingual word embeddings such as 
% Low-resource scenario 
% dictionary is more widely available than parallel data 
% should test it on dependency parsing task  => OK 
% Test Bilingual Lexicon Induction task (some overlaping with test set)

\includepdf[pages={1-},offset=0mm 0mm,scale=1,pagecommand={}]{Papers/emnlp16.pdf}

%------------------------------------------------------------%
%------------------------------------------------------------%
%------------------------------------------------------------%

\section{EACL 2017 -- Multilingual Word Embeddings}
\label{sec:eacl17}
\begin{quote}
Long Duong, Hiroshi Kanayama, Tengfei Ma, Steven Bird, Trevor Cohn. Multilingual Training of Crosslingual Word Embeddings (to appear). In \textit{Proceedings of the 2017 Conference on European Chapter of the Association for Computational Linguistics (EACL 2017)}. Valencia, Spain. 
\end{quote}

\subsubsection{Research process}
This work is the extension of our \emnlpvi\ paper, also conducted during my time at IBM Research Tokyo. We extended crosslingual word embeddings to more than two languages. Also, we better position our work relative to prior work using more evaluation metric focusing on downstream applications. 

In this paper, I implemented the algorithm and ran all experiments. Other co-authors (my supervisors) contributed ideas during our meetings and participated in the paper writing. 

\subsubsection{Retrospective view}
We have successfully extended the model for multilingual training of crosslingual word embeddings. However, this paper didn't compare the training complexity. 
The training complexity of both post-hoc unification methods (e.g. linear transformation or CCA) and multilingual join training grows linearly with the 
number of languages. However, post-hoc training is much easier to parallelize since each language can be trained separately. Scaling up multilingual join 
training to massive number of languages (e.g. 100 languages) is definitely be more challenging. In addition, while post-hoc unification method's performance 
is constant regardless of number of languages, it is an open question on how the performance changes with multilingual join training when  more languages are added. 

\includepdf[pages={1-},offset=0mm -7mm,scale=0.95,pagecommand={}]{Papers/eacl17.pdf}

%------------------------------------------------------------%
%------------------------------------------------------------%
%------------------------------------------------------------%

\section{NAACL 2016 -- Speech Translation}
\label{sec:naacl16}
\begin{quote}
Long Duong, Antonios Anastasopoulos, David Chiang, Steven Bird and Trevor Cohn. An Attentional Model for Speech Translation Without Transcription. In\textit{ Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2016)}. 949--959, San Diego, USA.
\end{quote}

\subsubsection{Research process}
This paper aim at solving the fourth task in our thesis, processing unwritten languages. This is a challenging task as we have to work directly on speech 
signal. We formalize the problem as sequence to sequence learning building upon the neural models of machine translation of~\namecite{DBLP:journals/corr/BahdanauCB14}. 

This paper is the outcome of the project started during my internship at ICSI, UC Berkeley. Aside from A.Prof. Steven Bird and Dr. Trevor Cohn, this is also the join work with Antonios Anastasopoulos and A.Prof. David Chiang from Notre Dame University. Anastasopoulos helped with preparing the dataset, evaluation scripts and calculating baselines. I implemented the algorithms and ran experiments adapting Dr. Cohn's work on neural machine translation~\cite{cohn-EtAl:2016:N16-1} for speech. Other co-authors contributed ideas during our meetings and participated in the paper writing. 

\subsubsection{Retrospective view}
Despite the fact that we achieve good results on phoneme level representation, the result for the experiment applied directly to speech data was unsatisfying for both 
translation and alignment quality. This is partly because of the small size of speech 
data and the data hungry nature of deep neural approach. While the idea is novel and interesting, the challenges are still very much open and our work is just a first step. I have co-authored another paper with Antonios Anastasopoulos and A.Prof. David Chiang (not included in this thesis), taking simpler approach using Expectation-Maximization which achieves better result~\cite{anastasopoulos-chiang-duong:2016:EMNLP2016}.

\includepdf[pages={1-},offset=0mm 0mm,scale=1,pagecommand={}]{Papers/naacl16.pdf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% NEW CHAPTER %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Discussions and Conclusions}
\label{chap:conclusion}
How can we analyze low-resource language with minimal or no training data? How can we model a language where even the writing system is not available? The previous chapters aimed to answer those questions. 
In this chapter, we conclude by returning to the questions raised in the introduction: (a) How did the publications in \S\ref{chap:research_summary} fit into the thesis's aim? (b) How did the research questions get 
answered? and (c) How well did we resolve the four low-resource tasks: POS tagging, dependency parsing, crosslingual word embeddings and unwritten language processing? Finally, we also critically evaluate the contributions and enumerate some possible directions
for future work. 

\section{Evaluation of Contribution}
\label{sec:evaluation_contribution} 
In this thesis, we have looked at different facets of processing low-resource languages. Our contributions are fourfold: 
\begin{itemize}
\item An algorithm to effectively incorporate complementary resources -- such as parallel corpora or language relatedness -- to low-resource languages. We exploit a transfer learning framework, taking advantage of a large annotated corpus in the source language and some annotations in the target language. We have shown that these methods can improve 
the performance for POS tagging even for high resource languages. 
\item An algorithm to improve the quality of fully unsupervised learning for low-resource languages, taking advantage of crosslingual syntactic embeddings and crosslingual 
word embeddings, applied successfully to dependency parsing and document classification. 
\item Algorithms to solve specific problems to low-resource languages, such as annotation mismatch between source and target language or extremely low-resource scenario of unwritten languages.
\item Several new datasets including English-Serbian bilingual lexicon induction dataset and speech to text alignment corpus. 
\end{itemize}

\subsection{Task coverage}
% COVER ALL 4 TASKS, what is task coverage ....
\noindent In the introduction (\S\ref{chap:introduction}), we proposed four low-resource NLP tasks including POS tagging, dependency parsing, cross lingual word embeddings and unwritten language processing. In this section, we review what we have done for each task. 
\paragraph{POS tagging} For the first task of low-resource POS tagging, in our \emnlpivp, we propose a novel semi-supervised method building 
upon our prior work on unsupervised POS tagging~\cite{duongIJCNLP,Duongacl13}. Compared with 
the previous state-of-the-art system, which also takes advantage of parallel data, we make a more realistic assumption, using less parallel data but achieving a better result. 
In that paper, we also propose a novel tagset mapping algorithm, applied when source and target language use different tagset inventory. At the time of writing the paper, we achieved the state-of-the-art performance on the low-resource language Malagasy. 

\paragraph{Dependency parsing} For the second task of dependency parsing, in our \aclvp, we proposed a semi-supervised method exploiting parameter sharing in the 
neural network parser between source resource-rich language and target resource-poor language. The model was trained in the cascade style 
where the source language model was trained first and used as the prior for the target low-resource language model. We shown that we can achieve  a more accurate parser using the same training data. In our \emnlpvp, we further improved performance in the semi-supervised setting. We modified 
the training such that source and target language parsers are jointly optimized, allowing better parameter sharing. We observed a consistent gain
of as high as 10\% across various data sizes in the low-resource setting. In that paper, we also showed that the word embeddings learned as part 
of the joint training capture meaningful syntactic phenomena, achieving high performance on the verb similarity task. In the extreme case, where 
we do not have any annotated treebanks, we proposed an unsupervised algorithm in our \conllvp\ taking advantage of the novel syntactic word 
embeddings without recourse to parallel data. When applied to the target low-resource language, we observed consistent improvement. In addition, 
if there are multiple source languages, we propose a method to combine them together which leads to substantial improvement. 

\paragraph{Crosslingual word embeddings}The third task of learning crosslingual word embeddings, is covered by EMNLP 2016 and EACL 2017 (\S\ref{sec:emnlp16}, \S\ref{sec:eacl17}) papers. Previous approaches have expensive resource requirements with a 
high complexity objective function or being unable to handle polysemy. We address those drawbacks in our \emnlpvip, taking advantage of high 
coverage but noisy dictionary, in an expectation-maximization style disambiguation training over monolingual data in two languages. We showed that the 
learned embeddings are of high quality, preserving both monolingual and bilingual distance. We achieved competitive results on unsupervised crosslingual 
document classification task without recourse to parallel corpora. Moreover, our proposed technique to combine embeddings during training can even be used for 
improving monolingual word embeddings. However, in the \emnlpvip, we only considered building crosslingual word embeddings for a pair of languages. 
It is more beneficial when we build crosslingual word embeddings for multiple languages, which is addressed in our \eaclviip. We proposed
several methods for building multilingual word embeddings including post-hoc combination and joint-training. Using our multilingual word embeddings, we observed improvement in both unsupervised crosslingual document classification and in unsupervised dependency parsing tasks. 

\paragraph{Unwritten language processing}The last proposed task of processing unwritten language was addressed as a preliminary investigation in our \naaclvi\ paper. As a first proof of concept, we decided 
to model directly the alignment between speech signal from low-resource language and the target higher resource language translation. We experimented with a neural attentional model 
for this data. Our initial experiments based on phoneme transcription in the source language, showed a large improvement relative to several 
baselines. On the more challenging speech-to-word modelling task, we showed that the model was still capable of learning meaningful relations establishing the feasibility of this proposed task. 

\subsection{Research questions revisit}
\noindent \textbf{Question 1}: How can we achieve more accurate models for low-resource languages using less annotated data? 
\begin{itemize}
\item In the \emnlpivp, we showed that bilingual corpora, when available, can be used to transfer the annotation from a source resource-rich language to a target 
resource-poor language. We observed improvements using this parallel corpus together with a small part-of-speech annotated corpus in the target language. Even if 
the annotation is different between source and target language, we can also automatically learn the mapping as part of training. 
\item 	In the ACL 2015 and EMNLP 2015 papers (\S\ref{sec:acl15}, \S\ref{sec:emnlp15}), we introduced the concept of ``universal parser`` where as there is a shared underlying linguistic structure across languages. 
Using this shared structure together with a small annotated corpus in the target language, we obtained a more accurate model.  
\end{itemize}

\noindent \textbf{Question 2}: How can we achieve a more accurate model for low-resource languages without any annotated data for those languages?

\begin{itemize}
\item In \conllvp, we show that even in the extreme case where there are not any annotated data, we can still improve the performance of unsupervised dependency parser by relexicalizing the 
delexicalized model using crosslingual syntactic word embeddings. In addition, we can even further improve the performance when multiple source languages are available, exploiting transfer learning using multiple source languages. 
\item Transfer learning has been a successful technique applied to improve performance on low-resource languages, in the \emnlpvip, we investigated a new way to build crosslingual word embeddings used for lexical transfer. Crosslingual word embeddings can be used in transfer learning applied for many unsupervised natural 
language processing tasks. In the \emnlpvip, we demonstrated its applicability for unsupervised crosslingual document classification. 

\item In the \eaclviip, we extended the crosslingual word embeddings training to multiple languages. We showed that multiple source languages can be used to further improve the performance for unsupervised learning and demonstrated this successfully for crosslingual document classification and crosslingual dependency parsing tasks.  
\end{itemize}

\noindent\textbf{Question 3}: What can we learn from unwritten languages?
\begin{itemize}
\item In our \naaclvip, we proposed the new task of learning directly from unwritten language speech together with the translation into a higher resource language. While this work is preliminary, we showed that meaningful alignments can be learned directly, demonstrating the feasibility of this approach. This work provides the first stepping stone for 
subsequent work. % such as~\namecite{anastasopoulos-chiang-duong:2016:EMNLP2016}. 
\end{itemize}

\section{Future Work}
\label{sec:futurework}
\noindent This section enumerates some possible directions for future work. 
\subsection{More evaluations for low-resource languages} 
A shortcoming of this thesis is that in most case we evaluated in a simulated low-resource scenario.
This is a common strategy for low-resource natural language processing research, since it is hard to obtain gold evaluation data 
for a real low-resource language. We are fortunate to have the Malagasy part-of-speech evaluation corpus for our \emnlpivp. 
For the dependency parsing task, we mainly evaluated on European languages, while Hungarian and Irish can be considered as  
low-resource languages, other languages such as Czech or French are not. 
The new version of universal dependency 
treebank v1.3~\cite{11234/1-1699} has some candidate low-resource languages such as Buryat, Coptic, Kazakh and Sanskrit
which have very modest treebank size (fewer than 1000 annotated sentences). We would like to evaluate our existing approaches (\S\ref{sec:acl15}, \S\ref{sec:emnlp15}, \S\ref{sec:conll15}) to those languages in both semi-supervised and unsupervised settings for testing the generalization. 

In our \emnlpvip, we included Serbian as an example of a low-resource language. The performance on the bilingual lexicon induction 
task on English-Serbian is still acceptable given the small size of dictionary and monolingual data. However, it would be preferable to evaluate  
on downstream applications for Serbian, such as dependency parsing or document classification. However, this would require manual 
test data annotation. Our \eaclviip\  proposes methods for multilingual training for crosslingual word embeddings, extending the
~\emnlpvip. However, in that paper, we only experimented with high resource European languages. For future work, we would 
like to extend this to more low-resource languages outside of European languages. 

In our \naaclvip, we experimented with Spanish speech and English translation. This is convenient as we can get Spanish phoneme transcription
for the upper bound experiments. However, it would be interesting to apply for real low-resource languages.~\namecite{Blachon201661} used 
an extended version of Aikuma~\cite{bird-EtAl:2014:W14-22} to collect more than 80 hours of speech and the translation from Congo-Brazzaville
language, which would make for a more convincing evaluation. 

\subsection{Real-world evaluation}
In common with most work in the literature, when working with low-resource languages, we make simplifying assumptions. 
In future work, we would like to systematically test and verify these assumptions in the real low-resource language scenario.
In our our work on POS tagging and dependency parsing, given a small treebank in the target language, we employ various approaches, incorporating 
complementary information to produce a more accurate model. However, we have not considered the cost of applying our proposed approaches to 
the model. %In fact, what we need to do is calculate the effort in term of man power or money to employing our techniques, 
%assuming it is $X\$$. We need to compare the performance between (a) use this $X\$$ to annotate more data and apply any simple supervised 
%learning and (b) using this $X\$$ to employ our proposed approaches. Only in this way, it would be a fair comparison. However, correctly 
Simply put, we want to quantify our contribution, not in terms of accuracy, but in terms of time or money which is the usual basic for allocating resources in real-world applications. 
%Instead of picking a task such as POS tagging or dependency parsing, and an acceptable accuracy, e.g. 90\%, and estimating the amount of time or money needed to finish the task with and without our proposed methods. 

\subsection{Filling in  the gap}
% Crosslingual word embeddings with our parsing model ...
In our \conllvp, we used crosslingual syntactic embeddings to improve performance of unsupervised dependency parsing. In our EMNLP 2016 and EACL 2017 paper (\S\ref{sec:emnlp16}, \S\ref{sec:eacl17}), we proposed crosslingual word embeddings using a bilingual dictionary. The simple extension is just to use our proposed crosslingual word embeddings instead of 
crosslingual syntactic embeddings. 

In our \emnlpvp, we jointly trained the dependency parser for a pair of languages. The output parser can be understood as combination of two parts. The universal
parser part that parses ``universal language'' and the conversion part that converts a language to the universal language. It would be interesting to jointly train the model with many source languages. In this way, we expect to truly learn a universal parser as contrast to the work of~\namecite{TACL892}.  

In our EMNLP 2016 and EACL 2017 paper (\S\ref{sec:emnlp16}, \S\ref{sec:eacl17}), we built the crosslingual word embeddings using dictionaries from PanLex. However, we can even further improve 
the dictionary coverage by combining PanLex with a bilingual dictionary from Wiktionary and a bilingual dictionary induced from parallel data. Since our model 
has the capability to disambiguate noisy dictionary translations, this will likely boost the performance. 

In our \naaclvip, there are several problems with the current model we would like to investigate. First we want to change to a global objective function 
instead of a current local objective function. Basically, the current objective function is to learn a language model which is conditioned on the speech signal. 
This local objective function cannot ensure the global quality of the generated translations. Second, the current training and testing procedures are not consistent. During training, the model is conditioned on the previous ground truth word to generate the next word. During testing, the model is instead conditioned on the generated words, which are prone to errors. These problems are also observed in~\namecite{wiseman-rush:2016:EMNLP2016} and~\namecite{DBLP:journals/corr/BahdanauBXGLPCB16}. Last, we want to extend the decoder so that it can memorize longer history, ensuring higher quality translations. 

\section{Conclusion}
In this thesis, we have proposed several methods for automatically processing low-resource languages. We have not covered all the natural language 
processing tasks, but the proposed methods using transfer learning are universal and applicable for many tasks. We considered low-resource
languages with different levels of resource requirements, ranging from semi-supervised learning, unsupervised learning and our preliminary attempt at unwritten languages. 
% Strength and Weakness of the thesis 
While the claims in this thesis are backed up by publications, further empirical evaluation is needed for real low-resource 
languages. We believe that when evaluating on real low-resource languages, many unexpected challenges will arise and need to be considered. 

% What is the taking home lesson
The take-home lessons are that processing low-resource languages is hard, but transfer learning is a compelling solution; there are many complementary resources we can use to compensate for the lack of annotated data; and unwritten languages are common, and should be included in any low-resource natural language processing system.  

% Impact on the field ? 
There are some
notable investigations that have benefited from our research: POS tagging~\cite{DBLP:conf/conll/FangC16,Pecheux2016,zhang-EtAl:2016:N16-13,bacskaya2016semi}, parsing~\cite{DBLP:journals/corr/GillickBVS15,Guo:2016:RLF:3016100.3016284,TACL892,DBLP:journals/corr/GuoCWL16,ledbetter-dickinson:2016:BEA11,TACL917}, crosslingual word embeddings~\cite{zhang2017bilingual,vulic-eacl:2017,heyman-etal-eacl:2017}, unwritten language processing~\cite{DBLP:journals/corr/BansalKGL16,adams-EtAl:2016:EMNLP2016,anastasopoulos-chiang-duong:2016:EMNLP2016,Wilkinson+2016}. We hope our research will continue to drive the field forward, becoming a stepping stone for future research on low-resource natural language processing. 

% Personal gain 
%Personally, I learned a lot from this PhD including graphical model, Bayesian inference, domain adaptation, structure prediction and deep neural network. I also expand my knowledge about more NLP tasks such as machine translation, POS tagging, dependency parsing, document classification, representation learning, word sense disambiguation, speech recognition, natural language understanding, language and vision. During my PhD, I also expand international collaborations through my two internships (at IBM Research -- Tokyo and ICSI, UC Berkeley), set of international conferences I attended (EMNLP 14, EMNLP 15, ACL 15, ConLL 15, EMNLP 16) and set of talks I gave (at Google, Ebay, IBM, Monash University, UC Berkeley and NII -- Tokyo). I also develop academic experience through teaching at CIS department, University of Melbourne and serving as program committee on several conferences such as EMNLP 15, ACL 16, NAACL 16, COLING 16 and ALTA 16. Looking back, I can say that I enjoyed my PhD. 

% 

% Chapter 2: Background and litereature review 
%\include{ch2}


% Chapter 3: Universal tagger 
%\include{ch3}


% Chapter 4: Source language(s) selection
%\include{ch4}



% Chapter 5: Summary and Future work
%\include{ch5}

% bibliography:
\footnotesize
\include{b}
\normalsize
%\appendix
%\chapter{Other Papers}
%this is other chapter
%\include{ap1}

% insert other appendices here...

\end{document}
