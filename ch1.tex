%% intro.tex
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{section:intro}


\section{Part-of-speech Tagging}
% What is POS 
A Part-of-speech (POS) is the syntactic category of a lexical item (word). Lexical items which share the same POS are believed to have similar syntactic and morphological behavior. Common POS include Verb, Noun, Adjective, Pronoun, Adverb, Preposition and so forth. 
% Usage of POS 
POS information is used to disambiguate different syntactic categories. For example, in the sentence ``\emph{We can can a can}", the word ``\textit{can}" belongs to different categories: (1)~a modal verb, ie, \textit{somebody can do something}; (2)~a verb which is making a bottle; (3)~a noun which refers to a container. A POS tagger is a system for automatically determining the POS tag for a given text, and it should be able to distinguish syntactic categories by assigning tags to words. The above sentence can be tagged as ``\emph{We$_N$ can$_{MD}$ can$_V$ a$_{Det}$ can$_N$}" where \textit{N} is noun, \textit{MD} is modal verb, \textit{V} is verb, \textit{Det} is determiner. Another example is the word ``\textit{running}", the common interpretation is a verb i.e. ``\textit{he is running}". However, it could be an adjective i.e. ``\textit{running shoes}"  or noun i.e. ``\textit{running is good for health}". Therefore, sometimes POS tagging is also referred as choosing the right sense for the right context. Nevertheless, it is worth noting that when the same word-form has different tags, it will differ in meaning, but the reverse statement is not always true. For example, the word ``\textit{bank}" might refer to (1) the place where people conduct monetary transaction (2) the place near the river, but both meanings have the same ``\textit{Noun}" POS. 

POS tagging is one of the most basic operations of computational linguistic. Since it helps to disambiguate syntactic categories (and possibly senses), POS are regularly used in various natural language processing (NLP) tasks such as parsing, sentence classifying, word sense disambiguation and so-forth. 

\section{Tag set}
% Common tag set 
Tag set is the list of possible tags that a lexical item (word) can have. Normally, the tag set is different across languages. For English, the well-known tag set is Penn Treebank \cite{PenTreeBank} tag set which contains 48 tags; for more information please see Appendix \ref{section:appendixA}. The tag set is used to disambiguate morphology and syntax of lexical items. Therefore, the number of tags denotes the morphological complexity of a language. For example, there are 67 classes in the Prague Dependency Tree Bank \cite{HajicHajicovaAl2000} which shows the greater morphological complexity of Czech language compared with English. 
% Ref to apendix 
%\section{Formal definition}
%POS Tagger is the function $ f:A^+ \rightarrow T $ where $A$ is the alphabet of phonemes, $A^+$ denotes any non-empty sequence of phonemes. Normally, phonemes equal to letters. $T$ refers to tag set 
% Formal definition 
\section{Evaluation}
The most straightforward evaluation for POS tagging is \emph{per-token} accuracy. Firstly, sentence is tokenized, which will separate lexicon items from each other. Normally, space is used as the delimiter and period ``." indicates a sentence boundary. However, there are cases where this will not work, for example, the sentence \textit{``Mr. Peter ate a banana"} should be tokenized as \textit{(Mr.) (Peter) (ate) (a) (banana)} but not \textit{(Mr) (.) (Peter) (ate) (a) (banana)}. Second, the whole sentence is tagged. The tag of each token is then compared with the gold standard test data which is manually annotated by a linguist. The percentage of correctly tagged for tokens is known as the per-token accuracy. This measure is widely applied and has become the default evaluation. Supervised POS taggers achieve as high as 97 \% per-token accuracy for English~\cite{Toutanova:2003} and for many other languages. 

People might argue that this per-token accuracy is not meaningful because it takes into account punctuation marks and tokens that are not ambiguous. Therefore, another evaluation metric is  \emph{ambiguous-token} accuracy which only counts tokens that have more than one possible tag. The list of possible tags for each word can be acquired using dictionary or large manually annotated data. To the best of our knowledge, the best system \cite{Yoshida:2007} reports ambiguous-token accuracy around 87\%. 

Other evaluation is \emph{per-sentence} accuracy. That is, accuracy is calculated on the number of sentences for which all tokens are correctly tagged. This measure is more meaningful for tasks such as dependency-parsing where a single mistake in POS tagging might lead to completely wrong parsed sentence. The current good taggers report per-sentence accuracy around 55-57 \% \cite{Manning:2011:100}. 
In this thesis, we employ \textbf{per-token accuracy} for comparison purposes, thus, when we report accuracy of a tagger, we implicitly mean per-token accuracy. 

\section{Traditional approach}
The traditional approach to POS tagging builds a separate tagger for each target language. It does not take into consideration the relationship between languages. The supervised method which employs supervised machine learning algorithms is usually used. For each language they collect and build a large amount of manually annotated data and train a supervised POS tagger on it. The supervised style for the traditional approach has achieved very high tagging accuracy. The only issue is handling the out-of-vocabulary (OOV) case. The tagger is trained on labeled training data. However, there are cases where lexical items in test data are not present in training data. Therefore, we have very little information to predict the tag of an OOV word. The straightforward solution for the OOV is based on frequency of tags. That is assigning the highest frequency tag for the OOV word. More advanced solution might use the suffix/prefix of word or the prior tag sequences. For example, the suffix \textit{-ly} i.e. \textit{beautifully, quickly, lovely} etc, indicates an adverb, while the suffix \textit{-ive} i.e. \textit{attractive, possessive} indicates an adjective. Also, the information about tag sequences indicates that after determiner is usually a noun. 

\section{Challenges in POS tagging}
% Difficulty in POS tagging 
% Lack of data 
The first challenge for POS tagging is the training data. Currently, all supervised taggers outperform unsupervised ones. Supervised algorithms for POS tagger perform as accurate as 97\% for English~\cite{maximumEntropy}, French~\cite{denis:inria-00614819} and many other languages. However, supervised learning needs manually annotated data which is time consuming and costly to construct. There are approximately 7000 languages in the world but very small fraction (around 30 languages) have sufficient POS manually annotated data for building reliable supervised POS tagger. Unsupervised POS tagging, on the other hand, does not need any manually annotated data. It is more or less equivalent with unsupervised clustering task. That is, they tried to group words into syntactic cluster. The hypothesis is that words in the same cluster must share the same POS. However, there is a huge gap between supervised and unsupervised learning accuracy.  

% Lack of consensus between languages 
Current POS taggers are language oriented; there is a lack of consensus about the tagset. Tag set are adapted to each language, \textit{ an obstacle for cross-language processing}. For example, when calculating syntactic similarity between two languages, we need to compare tag sequence similarity. However, since tag sets for each language are different, they are incomparable. Another example is when working with a multilingual environment such as the World Wide Web, giving a solution that can work for every language is in high demand. However, if we keep individual tagsets for each language, we might have to manually or semi-automatically map tagsets between languages pairwise.  

\section{Universal Tagger}
% TODO : Introduce about 
In this thesis, we are going to investigate a solution to address the current challenges for POS tagging using a \emph{Universal Tagger}. Supervised styles, as mentioned above, are impossible for our Universal Tagger, which aims at building taggers not only for resource-rich but also for resource-poor languages. Completely unsupervised style is possible but low accuracy makes unsupervised POS tagger impractical. In this paper, we would like to investigate an unsupervised approach but additionally, exploit parallel data to copy tag information from resource-rich to resource-poor languages. Parallel data is acquired from a parallel corpus which contains many pairs of source sentences and their translation to the target language. We can use parallel data to bridge between languages and transfer annotated data from one language to the other. The intuition is that, for many resource-poor languages, there is no manually POS annotated data. Such data involves the intensive work of linguist, but parallel data is easier to acquire. 

The development of parallel data motivates us to build the Universal Tagger. Multilingual government documents, film subtitles, and a large amount of translation memory from books and news-papers, which are the source for parallel data, are becoming more and more widely available. Not only the size but also the language coverage of parallel data has improved drastically. The era of English dominating one side of parallel data is shifting to a far wider range of languages. 

Moreover, the Universal Tagger aims at pushing the boundary for cross-lingual natural language processing (NLP). Thus, the output of the Universal Tagger for each language must be comparable. To do this, we must employ the same tagset across languages. In this ways, we resolve the traditional ``language oriented" issues of the conventional POS tagger. 

\section{Main contributions}
In this thesis, we concentrate on the single task of unsupervised multilingual POS tagging (Universal  Tagger) which exploits parallel data. We give thorough reviews at related works on both traditional approach (monolingual) and modern approach (multilingual). We show both strong and weak points of these approaches in Chapter \ref{chap:backGround}. 

On Chapter \ref{chap:uniTagger} we are going to build a Universal Tagger which employs the consensus 12 Universal Tagset \cite{UniversalTagSet} which will enable cross-lingual comparison. Given a tagger for the source language and source-target language parallel data, we are able to build a tagger for the target language. We evaluate this on the same test data on the same 8 languages with the-state-of-the-art~\cite{Das:2011} using the same per-token accuracy metric. On average, our system performs on par with theirs but is penalized by using less data. Moreover, our method is much less sophisticated and runs much faster. 

Out of 8 languages, we perform better at 4 languages which are in the same language family tree (Germanic) with source language (English). This fact motivates us for Chapter \ref{chap:sourceLanguageSelection} which we dedicate for investigating the effectiveness of choosing different source languages. It turns out that English is rarely the best source language for many cases. We are also able to build a model that can predict the best source language just based on monolingual data. This model performs better than always fixing a single best source language. The further accurate prediction can be acquired if we additionally use multilingual data from the parallel corpus. Finally, we show that, if multiple source languages are available we can even get further improvement by incorporating information from multiple source languages.

Chapter \ref{chap:conclusion} contains the conclusion and a discussion of future work. We organize future work into different categories chronologically. That is: (1)~immediate work which might take few weeks to finish; (2)~near future work which might take a few months, and (3)~longer future work which might take years. 
