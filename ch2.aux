\relax 
\citation{PenTreeBank}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background and Literature Review}{6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:backGround}{{2}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Monolingual POS Tagger}{6}}
\citation{Brill95transformation}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces List of best English taggers (all $>96\%$ on WSJ test set)}}{7}}
\newlabel{tab:eng_state_of_the_art}{{2.1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Supervised}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Transformation Based Tagging - a rule based approach}{7}}
\@writefile{toc}{\contentsline {paragraph}{The learner}{7}}
\citation{vietnameseTagger}
\citation{Brill95transformation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Learner component of TBT }}{8}}
\newlabel{fig:tbt_learner}{{2.1}{8}}
\@writefile{toc}{\contentsline {paragraph}{The Tagger}{8}}
\citation{REINSCH1967}
\citation{Gale94goodturing}
\citation{Ryan:1993}
\citation{beamsearch}
\@writefile{toc}{\contentsline {subsubsection}{Hidden Markov Model - a probabilistic approach}{9}}
\newlabel{equa:chainRule}{{2.1}{9}}
\newlabel{equa:assump1}{{2.2}{9}}
\newlabel{equa:assump2}{{2.3}{9}}
\newlabel{equa:hmmFinalEquation}{{2.4}{9}}
\citation{TNTTagger}
\citation{maximumEntropy}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Emission and transition probabilities. $W_i$ is a word and $T_i$ is a tag}}{10}}
\newlabel{fig:hmmTagger}{{2.2}{10}}
\@writefile{toc}{\contentsline {subsubsection}{Maximum Entropy - a feature based approach}{10}}
\citation{Darroch1972}
\newlabel{equa:loglinearMaxEnt}{{2.5}{11}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Finding $\lambda _i$}}{11}}
\newlabel{alg:findLambda}{{1}{11}}
\citation{beamsearch}
\citation{Ryan:1993}
\citation{Toutanova:2003}
\citation{Toutanova03}
\citation{Tsuruoka:2005}
\citation{svmtool}
\citation{shiftReduce}
\citation{ParsingAndTagging}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Steps to build a classifier}}{12}}
\newlabel{fig:machineLearning}{{2.3}{12}}
\newlabel{equa:finalMaxEnt}{{2.6}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Other algorithms}{12}}
\citation{disPOSTagging}
\citation{tfidf}
\citation{disPOSTagging}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Constituency Parse Tree}}{13}}
\newlabel{fig:parsingTagging}{{2.4}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Unsupervised}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Clustering}{13}}
\citation{ManyToOneEvaluate}
\citation{unSupPOSClustering}
\citation{chineseWhisper}
\citation{unSupPOSClustering}
\citation{semiSuPOSCondense}
\citation{BaumWelch}
\citation{forwardBackward}
\citation{featurebaseHMM}
\citation{featurebaseHMM}
\@writefile{toc}{\contentsline {subsubsection}{HMM unsupervised tagging}{15}}
\newlabel{hmmUnsup}{{2.1.2}{15}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Baum-Welch algorithm}}{15}}
\newlabel{alg:baumWelch}{{2}{15}}
\citation{Das:2011}
\citation{triTraining}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Comparison between with and without dictionary unsupervised POS tagger for 8 languages (Danish(da), Dutch(nl), German(de), Greek(el), Italian(it), Portuguese(pt), Spanish(es), Swedish(sv) }}{16}}
\newlabel{tab:superviseUnsupervise}{{2.2}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Semi-supervised}{16}}
\@writefile{toc}{\contentsline {subsubsection}{Self-training, Tri-training approach}{16}}
\citation{semiSimple}
\citation{svmtool}
\citation{Toutanova:2003}
\citation{semiSuPOSCondense}
\citation{semiSuPOSCondense}
\citation{svmtool}
\citation{unSupPOSClustering}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Comparing self-training, tri-training and tri-training with disagreement}}{17}}
\newlabel{tab:triTraining}{{2.3}{17}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Tri-training algorithm}}{17}}
\newlabel{alg:triTraining}{{3}{17}}
\citation{semiSuPOSCondense}
\citation{semiSuPOSCondense}
\citation{semiSuPOSCondense}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Sample data point in train data}}{18}}
\newlabel{tab:sampleDataPoin}{{2.4}{18}}
\@writefile{toc}{\contentsline {subsubsection}{Semi-supervised condensed nearest neighbor}{18}}
\citation{YarowskyAndNgai}
\citation{XiAndHua}
\citation{SnyderMultilingualPOS}
\citation{Das:2011}
\citation{Hana04}
\citation{Feldman06}
\citation{reddy2011crosspos}
\citation{reddy2011crosspos}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Condense Nearest Neighbor algorithm}}{19}}
\newlabel{alg:condenseNN}{{4}{19}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Semi-Supervised Weakened Condense Nearest Neighbor algorithm}}{19}}
\newlabel{alg:semiSupcondenseNN}{{5}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Multilingual POS Tagger}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Typologically related languages approach}{19}}
\citation{reddy2011crosspos}
\citation{CorpusFactory}
\citation{reddy2011crosspos}
\citation{SnyderMultilingualPOS}
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces Various tagging model exploit topologically related language}}{21}}
\newlabel{tab:topologyRelatedLang}{{2.5}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Unsupervised simultaneously tagging approach}{21}}
\newlabel{equa:simulTagging}{{2.7}{21}}
\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces Per-token tagger accuracy with gold dictionary for unsupervised monolingual and bilingual simultaneously tagging}}{22}}
\newlabel{tab:simultaneouslyTagging}{{2.6}{22}}
\@writefile{lot}{\contentsline {table}{\numberline {2.7}{\ignorespaces Per-token accuracy with reduced gold dictionary size for unsupervised monolingual and bilingual simultaneously tagging. Huge improvements are in bold}}{22}}
\newlabel{tab:simultaneouslyTaggingReducedDic}{{2.7}{22}}
\citation{YarowskyAndNgai}
\citation{Das:2011}
\citation{Das:2011}
\citation{featurebaseHMM}
\@writefile{lot}{\contentsline {table}{\numberline {2.8}{\ignorespaces Features used for computing similarity between trigram vertex}}{23}}
\newlabel{tbl:featuresForSimilarity}{{2.8}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Tag projection approach}{23}}
\citation{Das:2011}
\citation{europarl}
\citation{UniversalTagSet}
\citation{Das:2011}
\@writefile{lot}{\contentsline {table}{\numberline {2.9}{\ignorespaces Accuracy of multiple models on 8 languages. The best unsupervised performance for each language is in bold}}{25}}
\newlabel{tab:dasTable}{{2.9}{25}}
\@setckpt{ch2}{
\setcounter{page}{26}
\setcounter{equation}{7}
\setcounter{enumi}{7}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{4}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{9}
\setcounter{float@type}{16}
\setcounter{algorithm}{5}
\setcounter{ALC@unique}{48}
\setcounter{ALC@line}{9}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{r@tfl@t}{0}
\setcounter{parentequation}{0}
\setcounter{lips@count}{0}
\setcounter{lstnumber}{1}
\setcounter{exmp}{0}
\setcounter{lstlisting}{0}
}
